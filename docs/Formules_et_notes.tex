\documentclass[]{book}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\usepackage[margin=1in]{geometry}
\usepackage{hyperref}
\hypersetup{unicode=true,
            pdftitle={Formules et notes},
            pdfauthor={Nicolas Bellemare},
            pdfborder={0 0 0},
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\usepackage{natbib}
\bibliographystyle{apalike}
\usepackage{longtable,booktabs}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
\let\oldparagraph\paragraph
\renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
\let\oldsubparagraph\subparagraph
\renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

%%% Use protect on footnotes to avoid problems with footnotes in titles
\let\rmarkdownfootnote\footnote%
\def\footnote{\protect\rmarkdownfootnote}

%%% Change title format to be more compact
\usepackage{titling}

% Create subtitle command for use in maketitle
\newcommand{\subtitle}[1]{
  \posttitle{
    \begin{center}\large#1\end{center}
    }
}

\setlength{\droptitle}{-2em}

  \title{Formules et notes}
    \pretitle{\vspace{\droptitle}\centering\huge}
  \posttitle{\par}
    \author{Nicolas Bellemare}
    \preauthor{\centering\large\emph}
  \postauthor{\par}
      \predate{\centering\large\emph}
  \postdate{\par}
    \date{2019-03-24}

\usepackage{tabularx}
\usepackage{hyperref}

\usepackage{amsthm}
\newtheorem{theorem}{Theorem}[chapter]
\newtheorem{lemma}{Lemma}[chapter]
\newtheorem{corollary}{Corollary}[chapter]
\newtheorem{proposition}{Proposition}[chapter]
\newtheorem{conjecture}{Conjecture}[chapter]
\theoremstyle{definition}
\newtheorem{definition}{Definition}[chapter]
\theoremstyle{definition}
\newtheorem{example}{Example}[chapter]
\theoremstyle{definition}
\newtheorem{exercise}{Exercise}[chapter]
\theoremstyle{remark}
\newtheorem*{remark}{Remark}
\newtheorem*{solution}{Solution}
\let\BeginKnitrBlock\begin \let\EndKnitrBlock\end
\begin{document}
\maketitle

{
\setcounter{tocdepth}{1}
\tableofcontents
}
\chapter*{Preface}\label{preface}
\addcontentsline{toc}{chapter}{Preface}

\chapter{Introduction actuariat 2}\label{introduction-actuariat-2}

\section{Théorème de la fonction
quantile}\label{theoreme-de-la-fonction-quantile}

\BeginKnitrBlock{theorem}
\protect\hypertarget{thm:fn-quantile}{}{\label{thm:fn-quantile} }

\begin{align*}
U &\sim Unif(0,1)\\
Y &=F_x^{-1}(u) \Rightarrow Y \sim X\\
F_Y(x) &=F_{F_X^{-1}(u)}(x)=F_X(x)\, \text{pour}\, x \in \mathbb{R}\\
\intertext{ainsi:}\\
X &=F_X^{-1}(u)
\end{align*}

Voir preuve \ref{preuves:fn-quantile}
\EndKnitrBlock{theorem}

\section{Espérance tronqué}\label{intro:espuxe9tron}

\begin{gather*}
\begin{align*}
E[X \times \mathrm{1}_{ \{ X \ge x \} }]& =\int_{-\infty}^\infty y \times \mathrm{1}_{ \{ y \ge x\}}f_X(y)\, dy \notag \\ 
& =\int_{-\infty}^x 0\times f_X(y) dy + \int_x^\infty y f_X(y)\,dy \notag \\
& =\int_x^\infty y f_X(y)\,dy 
\end{align*}
\end{gather*}

\section{Fonction Stop-Loss}\label{intro:fn-stop}

\[
    \Pi_X(d) = E\left[\max(X-d,0)\right],\quad\text{pour}\,d \,\in \mathbb{R}
\] Voir preuve \ref{preuves:fn-stop}

\subsection{Variable continue}\label{variable-continue}

\[
    \Pi_X(d) = \int_0^\infty \max(X-d, 0)\,f_X(x)\,dx
\]

\subsection{\texorpdfstring{Variable discrète sur
\(({0,1h,2h,\dots})\)}{Variable discrète sur (\{0,1h,2h,\textbackslash{}dots\})}}\label{variable-discrete-sur-01h2hdots}

\begin{gather*}
\begin{align*}
f_X(kh) =& P(X=kh),\, k\in \mathrm{N},\, h > 0,\, d =k_0 h\\
\\
\Pi_X(k_0 h) =& E[\max(X-k_0 h, 0)]\\
=&\sum_{k=0}^\infty \max(kh-k_0 h, 0) P(X=kh)\\
=&\sum_{k_0=k+1}^\infty (kh-k_0 h) P(X=kh)\\
\end{align*}
\end{gather*}

\subsection{Propriété}\label{propriete}

\begin{gather*}
\begin{align*}
\Pi_X(0) =& \lim_{d \to 0} \Pi_X(d)\\
=& \lim_{d \to 0} E[\max(X-d, 0)]\\
=& E[X] \\
\end{align*}
\end{gather*}

\section{Fonction quantile}\label{intro:fn-quantile}

\subsection{Première forme}\label{intro:fn-quantile:1}

\begin{gather*}
\begin{align*}
\int_k^1 F_X^{-1}(u)\,du& =\int_k^1 \left[F_X^{-1}(u)-F_X^{-1}(k)+F_X^{-1}(k)\right]\,du\\
& =\int_k^1\left(F_X^{-1}(u)-F_X^{-1}(k)\right)\,du + F_X^{-1}(k)\int_k^1 (1)\,du\\
& =\int_0^1\max\left(F_X^{-1}(u)-F_X^{-1}(k),\, 0\right)\, du + F_X^{-1}(k)(1-k)\\
& = E\left[\max(F_X^{-1}(u)-F_X^{-1}(k),\, 0)\right]+(1-k)F_X^{-1}(k)\\
& = E\left[\max(X-F_X^{-1}(k),\, 0)\right]+(1-k)F_X^{-1}(k)
\end{align*}
\end{gather*}

\subsection{Deuxième forme}\label{intro:fn-quantile:2}

\begin{gather*}
\begin{align*}
\int_k^1 F_X^{-1}(u)\, du& =\Pi_X\left(F_X^{-1}(k)\right)+(1-k)F_X^{-1}(k)\\
\text{En remplaçant $\Pi_X(F_X^{-1}(k))$ par}\; \ref{preuves:fn-stop}\;\text{on obtient:}\\
& =E\left[X \times \mathrm{1}_{\{X > F_X^{-1}(k)\}}\right]-F_X^{-1}(k)\bar{F}_X\left(F_X^{-1}(k)\right)+(1-k)F_X^{-1}(k)\\
& =E\left[X \times \mathrm{1}_{\{X > F_X^{-1}(k)\}}\right]+F_X^{-1}(k)\left(F_X(F_X^{-1}(k))-k\right)
\end{align*}
\end{gather*}

\section{Fonction quantile et espérance}\label{fn-et-Ex}

\begin{gather*}
\int_0^1 F_X^{-1}(u)\,du =E\left[F_X^{-1}(x)\right]\\
\int_0^1 F_X^{-1} (u)(1)\,du = E[X]\\
\intertext{Généralisation:}\\
\int_0^1 \phi(F_X^{-1}(u))\,du =E[\phi(F_X^{-1}(u))] =E[\phi(X)]
\end{gather*}

\section{TVaR}\label{intro:tvar}

\[
\text{VaR}_k(X)= F_X^{-1}(k)
\]

\[
\text{TVaR}_k(X)= \frac{1}{1-k}\int_k^1\text{VaR}_u(X)\,du
\]

\subsection{Expression alternative 1}\label{intro:tvar:alt1}

\[
\text{TVaR}_k(X)= \frac{1}{1-k}\Pi_X\left(\text{VaR}_k(X)\right)+\text{VaR}_k(X)
\] Voir preuve \ref{preuves:tvar:1}

\subsection{Expression alternative 2}\label{intro:tvar:alt2}

\[
\text{TVaR}_k(X)= \frac{1}{1-k}\left(E\left[X\times\mathrm{1}_{\{X>{\text{VaR}_k(X)}\}}\right]+\text{VaR}_k(X)\times\left(F_X\left[\text{VaR}_k(X)\right]-k\right)\right)
\] Voir preuve \ref{preuves:tvar:2}

\subsection{Expression alternative 3}\label{intro:tvar:alt3}

\[
\text{TVaR}_k(X)= \frac{P\left(X\ge \text{VaR}_k(X)\right)}{(1-k)} \times E\left[X|X \ge \text{VaR}_k(X)\right]+\left(1-\frac{P\left(X\ge \text{VaR}_k(X)\right)}{(1-k)}\right)\times\text{VaR}_k(X),\quad k\in (0,1)
\] Voir preuve \ref{preuves:tvar:3}

\subsection*{Propriété}\label{propriete-1}
\addcontentsline{toc}{subsection}{Propriété}

\subsubsection*{Sous-additivité}\label{sous-additivite}
\addcontentsline{toc}{subsubsection}{Sous-additivité}

\label{intro:tvar:prop:sousadd} Soit \(S=X_1+X_2\), \[
\text{TVaR}_{\kappa}(S)\le \text{TVaR}_{\kappa}(X_1)+\text{TVaR}_{\kappa}(X_2)
\] Voir \autoref{preuves:tvar:prop:sousadd}

\section{Transformée de Laplace}\label{transformee-de-laplace}

Existe pour toute loi de X.

Lien avec \(E[X]\):

\begin{align*}
\text{V.A. X positive tel que}\; E[X]<\infty& \\ 
(-1)\frac{d}{dt}\mathcal{L}_X(t)\vert_{t=0}& =(-1)\frac{d}{dt}E\left[e^{-tX}\right]\vert_{t=0}\\
& =(-1)E\left[\frac{d}{dt}e^{-tX}\right]\vert_{t=0}\\
& =(-1)E\left[-Xe^{-tX}\right]\vert_{t=0}\\
& =(-1)E[-X] = E[X]
\end{align*}

Lien avec \(E[X^m]\): \[
E[X^m]=(-1)^m\frac{d^m}{dt^m}\mathcal{L}_X(t)\vert_{t=0}
\]

\section{\texorpdfstring{Bénéfice de mutualisation à mutualiser les
risques en utilisant la
\(TVaR\):}{Bénéfice de mutualisation à mutualiser les risques en utilisant la TVaR:}}\label{benefice-de-mutualisation-a-mutualiser-les-risques-en-utilisant-la-tvar}

\[
\text{BM}^{\text{TVaR}}_\kappa(X_1,\dots,X_n)=\sum^n_{i=1}\text{TVaR}_\kappa(X_i) -\text{TVaR}_\kappa(S_n)\geq0\;\kappa\in(0,1)
\]

Soit \(S_n=\sum^n_{i=1}(X_i)\), un portefeuille de risques identiquement
distribués(indep. ou pas), et \(W_n=\frac{1}{n}S_n\), la part des coûts
totaux par risque.\\
Soit \(\rho\), une mesure de risque qui satisfait les propriétés de
sous-additivité et d'homogénéité.\\
On déduit que

\begin{align*}
\rho(W_n)& = \rho\left(\frac{1}{n}S_n\right)\\
& =\rho\left(\frac{1}{n}\sum_{i=1}^n (X_i)\right)\\
\text{Par homogénéité:}\\
& =\frac{1}{n}\rho(S_n)\\
\text{Par sous-additivité:}\\
& \leq\frac{1}{n} \sum^n_{i=1}\rho(X_i)\\
& =\frac{1}{n} \sum_{i=1^n}\rho(X)\\
& =\frac{n}{n}\rho(X)\\
& =\rho(X)
\end{align*}

Cette relation est intéressante si on utilise
\(\rho=\text{TVaR}_\kappa (X)\) et si on utilise \(\rho_\kappa(W_n)\)
pour calculer la prime pour un risque d'un portefeuille homogène.

Soit une mesure de risque qui satisfait la propriété de convexité,ie \[
\rho(\alpha_1 X_1+\dots + \alpha_n X_n) \leq \sum^n_{i=1} \alpha_i\rho\left (X_i\right ),
\] pour
\(\alpha \geq 0,\;i=1,2,\dots,n\;\text{et}\;\sum^n_{i=1} \alpha_i=1\)

Posons \(\alpha_i=\dfrac{1}{n},\;i=1,\dots,n\)\\
Donc,

\begin{align*}
\rho(W_n)& = \rho(\dfrac{1}{n}X_1+\dots+\dfrac{1}{n} X_n)\\
& \leq \dfrac{1}{n} \rho(X_1)+\dots+\dfrac{1}{n}\rho(X_n)\\
& =\dfrac{1}{n}(\rho(X)+\dots+\rho(X))\\
& =\dfrac{1}{n}n \rho(X) = \rho(X)
\end{align*}

\(\rho(W_n)\leq \rho(X)\) si \(\rho\) satisfait la propriété de
convexité.

\chapter*{Modèles de risque non-vie}\label{modeles-de-risque-non-vie}
\addcontentsline{toc}{chapter}{Modèles de risque non-vie}

\section{\texorpdfstring{Modèle de base pour
\(X\)}{Modèle de base pour X}}\label{modele-de-base-pour-x}

\begin{enumerate}
\item V.A. $M=$ nombre de sinistres pour un risque  
\item V.A. $B_k=$ montant du sinistre $k,<;k<in\mathrm{N}^+$
\end{enumerate}

Modèle fréquence sévérité pour \(X\) où \(X=\) coûts pour un risque

\[
X=\begin{cases}
0,& M=0\\
B_1,& M=1\\
B_1+B_2,& M=2\\
B_1+B_2+B_3,& M=3\\
\quad\cdots\\
\end{cases}
\]\\
\[
X=\begin{cases}
0,& M=0\\
\sum^M_{k=1}B_k,& M > 0\\
\end{cases}
\] \(X\) est un somme de nombre aléatoire de V.A.

\subsection{Hypothèses
traditionnelles}\label{hypotheses-traditionnelles}

\begin{enumerate}
\item $\underline{B}=B_k,\;k\in \mathrm{N}^+$ forme une suite de V.A. indépendantes    
\item $\underline{B}:$ forme une suite de V.A. identiquement distribuées.   
\item Convention $B_k \sim B,\; k \in \mathrm{N}$   
\item $\underline{B}$ est indépendante du nombre de sinistre $M$    
\end{enumerate}

\subsection*{Précisions}\label{precisions}
\addcontentsline{toc}{subsection}{Précisions}

\begin{enumerate}
\item H1: Les montants des sinistres sont supposés mutuellement indépendants    
\item H2: Le montant de sinistre $k_1$ se comporte comme le montant de sinitre $k_2,\;k_1 \not{=} k_2$  
\item H3: LE nombre de sinistre n'a pas d'impact sur les montants de sinistres  
\end{enumerate}

\section{\texorpdfstring{Espérance de
\(X\)}{Espérance de X}}\label{esperance-de-x}

\(\text{E}[M]<\infty,\; \text{E}[B]<\infty\)

\[\text{E}[X]=\text{E}[M]\text{E}[B]\]\\
On conditionne sur le nombre \(M\) de sinistres \[
\text{E}[X]=\text{E}_M\left[\text{E}[X|M]\right]
\]\\
Où \(\text{E}[X|M]=\) V.A.= espérance des coûts pour le risque \(X\)
conditionnelle au nombre \(M\) de sinistres.

\begin{align*}
\text{E}[X|M=0]& =0\\
\text{E}[X|M=1]& = \text{E}[B_1]=\text{E}[B]\\
\text{E}[X|M=2]& = \text{E}[B_1+B_2]=\text{E}[B_1]+\text{E}[B_2]=2\text{E}[B]\\
\text{E}[X|M=k]& = \text{E}[B_1+\dots+B_k]=\text{E}[B_1]+\dots+\text{E}[B_k]=k\text{E}[B]\\
\text{Donc,}\\
\text{E}[X|M]& =M\text{E}[B]
\end{align*}

Alors,

\begin{align*}
\text{E}[X]& =\text{E}_M\left[\text{E}[X|M]\right]\\
\text{E}\left[M\text{E}[B]\right]& =\text{E}[M]\text{E}[B]
\end{align*}

Coûts espérés pour un risque \(X=\) (nombre espéré de sinistres)(montant
espéré d'un sinistre). Cette relation est valide que si H3 est posée.

\section{\texorpdfstring{Variance de
\(X\)}{Variance de X}}\label{variance-de-x}

\(\text{E}[B^M]<\infty,\; m=1,2\; \text{et}\; \text{E}[M^m]<\infty,\;m=1,2\)

On conditionne sur \(M\)\\
\[
Var(X)=\text{E}_M\left[Var(X|M)\right]+Var\left (\text{E}[X|M]\right )
\]

\begin{align*}
Var(X|M=0)& =0\\
Var(X|M=1)& =Var(B_1)=Var(B)\\
Var(X|M=2)& =Var(B_1+B_2)=Var(B_1)+Var(B_2)=2Var(B)\\
\text{Donc,}\\
Var(X|M)& =M\times Var(B)
\end{align*}

Alors,

\begin{align*}
Var(X)& =\text{E}\left [M\times Var(B)\right ]+ Var\left (M\times \text{E}[B]\right )\\
& =\text{E}[M]Var(B)+Var(M)\text{E}^2[B]
\end{align*}

La variance de \(X\) est expliquée par 2 sources:

\begin{enumerate}
\item Variabilité associée à la fréquence 
\item Variabilité asssociée au montant de sinistre
\end{enumerate}

La loi d'une V.A. \(X\) définie par une somme aléatoire de V.A. est
appelée une loi composée.\\
Les lois de bases pour \(M\):

\begin{enumerate}
\item Loi de Poisson
\item Loi binomiale négative 
\item Loi binomiale
\end{enumerate}

Loi de base pour \(B\):

\begin{enumerate}
\item Loi Gamma
\item Loi lognormale
\item Loi Pareto
\end{enumerate}

\section{Fonction de répartition}\label{fonction-de-repartition}

On suppose que \(B\) est une V.A. positive. On conditionne sur \(M\).

\begin{align*}
F_X(x)& = P(X\leq x)\\
& = \sum^\infty_{k=0} P(X \leq x|M=k) \times P(M=k)\\
& =P(M=0) + P(B_1 \leq x) \times P(M=1)\\
& +P(B_1+B_2 \leq x) \times P(M=2)+ \dots\\
& =P(M=0) + \sum^\infty_{k=1} P(M=k) \times P(B_1+ \dots + B_k \leq x)\\
& =P(M=0) + \sum^\infty_{k=1} P(M=k) \times F_{B_1+ \dots +B_k} (x)   
\end{align*}

\begin{enumerate}
\item ${F}_X$ est intéressante si on connait l'expression de $F_{B_1+ \dots +B_k} (x)$
\item Si $B_1 \sim \dots \sim B_k \sim Gamma$, on sait que $B_1+ \dots +B_k \sim Gamma(k\alpha,\beta)$
\item Si $B \sim LN$ ou Pareto, on ne sait pas l'expression de $F_{B_1+ \dots +B_k} (x)$
\item Pour la somme de 1 à l'$\infty$, on fixe $k_0$ tel que $\bar{F}_M(k_0)<\epsilon$,ex:$(10^{-7})$.
\end{enumerate}

\section{\texorpdfstring{FGP de \(M\)}{FGP de M}}\label{fgp-de-m}

\[
P_M(S)=\text{E}[S^M]=\sum^\infty_{k=0} P(M=k)S^k 
\] où \(S\in [0,1]\):les valeurs de \(S\) où la somme converge.

\section{\texorpdfstring{TLS de \(B\)}{TLS de B}}\label{tls-de-b}

\[
\mathcal{L}_B(t)=\text{E}[e^{-tB}],\;\text{pour}\;t\geq 0
\]

\section{\texorpdfstring{TLS de \(X\)}{TLS de X}}\label{tls-de-x}

\[
\mathcal{L}_X(t)=\text{E}[e^{-tX}]
\] On conditionne sur \(M\): \[
\mathcal{L}_X(t)= \text{E}_M\left [\text{E}[e^{-tX}|M]\right ]
\]

\begin{align*}
\text{E}[e^{-tX}|M=0]& =1\\
\text{E}[e^{-tX}|M=1]& =\text{E}[e^{-tB_1}|M=1]\\
& = \text{E}[e^{-tB_1}] \rightarrow \text{indépendante de }M\\
& = \text{E}[e^{-tB}] \rightarrow B_1 \sim B\\
& = \mathcal{L}_B(t)\\
\text{E}[e^{-tX}|M=2]& = \text{E}[e^{-t(B_1+B_2)}|M=2]\\
& = \text{E}[e^{-t(B_1+B_2)}]\\
& =\text{E}[e^{-tB_1} e^{-tB_2}]\\
& =\text{E}[e^{-tB_1}]\text{E}[e^{-tB_2}]\\
& =\text{E}^2[e^{-tB}]\\
& = \mathcal{L}_B^2(t)\\
\end{align*}

Donc,

\begin{align*}
\text{E}[e^{-tX}|M]& =\left (\mathcal{L}_B(t)\right )^M
\end{align*}

Alors,

\begin{align*}
\mathcal{L}_X(t)& =\text{E}\left [\left (\mathcal{L}_B(t)\right )^M \right ]\\
& = P_M\left (\mathcal{L}_B(t)\right ),\;t \geq 0
\end{align*}

\section{\texorpdfstring{VaR de \(X\)}{VaR de X}}\label{var-de-x}

Supposons, \[F_{B_1}(0)=F_{B_2}(0)=\dots=F_B(0)=0\]

Alors, \[
F_X(0) = P(M=0)+\sum^\infty_{k=1} P(M=k)F_{B_1+\dots+B_k}(0)=P(M=0) 
\]

Ainsi, \[
F_X^{-1}(u)=0,\;u\in(0,F_X(0)]
\]\\
Pour \(u\in(F_X(0),1)\) et si \(P(M>1)>0\), il faut un outil
d'optimisation pour la majorité des V.A. de \(M\) pour inverser \(F_X\).
Généralement, on peut évaluer \(F_X\) quand \(B \sim Gamma\) ou
\(B \sim invGauss\).

\section{Espérance tronquée}\label{esperance-tronquee}

\begin{align*}
\text{E}\left [X \times 1_{\{X>b\}}\right ]& =\text{E}\left [\text{E}\left [X \times 1_{\{X>b\}}|M\right ]\right ]\\
& =\sum^\infty_{k=0} \text{E}\left [X \times  1_{\{X>b\}}|M=k\right ]P(M=k),\;b\geq 0\\
& =P(M=0)(0)+P(M=1)\text{E}\left [B_1 \times   1_{\{B_1>b\}}|M=1\right ]\\
& +P(M=2)\text{E}\left [B_1+B_2 \times   1_{\{(B_1+B_2)>b\}}|M=2\right ]\\
& +P(M=3)\text{E}\left [(B_1+B_2+B_3) \times   1_{\{(B_1+B_2+B_3)>b\}}|M=3\right ]\\
&+ \dots\\
&+ \sum^\infty_{k=1} P(M=k) \text{E}\left [(B_1+ \dots +B_k) \times  1_{\{(B_1+\dots+B_k)>b\}}\right ]
\end{align*}

Cette expression est intéressante si on connait la loi de
\(B_1+ \dots +B_k\) ou si on peut évaluer
\(\text{E}\left [(B_1+ \dots +B_k) \times 1_{\{(B_1+\dots+B_k)>b\}}\right ]\).
On peut utiliser l'expression quand \(B \sim Gamma\) ou
\(B \sim invGauss\), mais on ne peut pas l'utiliser si \(B \sim Pareto\)
ou \(B \sim LNorm\) et quand \(P(M>1)>0\).

\section{\texorpdfstring{TVaR de \(X\)}{TVaR de X}}\label{tvar-de-x}

\begin{align*}
\text{TVaR}_\kappa (X)& = \dfrac{1}{1-\kappa}\text{E}\left [X\times 1_{\{X>\text{VaR}_\kappa (X) \}}\right ]\\
& + \dfrac{1}{1-\kappa}\text{VaR}_\kappa (X) \left (F_X(\text{VaR}_\kappa (X) )-\kappa\right )
\end{align*}

On suppose que \(B\) suit une loi continue avec \(F_B(0)=0\).\\
On fixe \(\kappa \in (0,F_{X}(0))\), \(\text{VaR}_\kappa (X) =0\).\\
Ainsi,

\begin{align*}
\text{TVaR}_\kappa (X)& =\dfrac{1}{1-\kappa}\text{E}\left [X\times 1_{\{X>0\}}\right ]\\
& =\dfrac{1}{1-\kappa}\text{E}[X]
\end{align*}

On fixe \(\kappa \in (F_{X}(0),1)\). Donc, \(\text{VaR}_\kappa (X) >0\)
et ses valeurs se trouvent dans la partie continue de \(X\).\\
Alors, \[
F_X\left (\text{VaR}_\kappa (X) \right )=\kappa
\]

On déduit

\begin{align*}
\text{TVaR}_\kappa (X)&=\dfrac{1}{1-\kappa} \text{E}\big [X\times 1_{\{X>\underbrace{\text{VaR}_\kappa(X) }_{\text{constante}}\}}\big ]\\
& =\dfrac{1}{1-\kappa}\sum^\infty_{k=1} P|(M=k) \text{E}\left [(B_1+ \dots+ B_k)\times 1_{\{(B_1+ \dots+ B_k)> \text{VaR}_\kappa(X) \}}\right ]
\end{align*}

\section{\texorpdfstring{Cas particulier
\(M \sim Bern(q)\)}{Cas particulier M \textbackslash{}sim Bern(q)}}\label{cas-particulier-m-sim-bernq}

\[X=\begin{cases}
B,& M=1\\
0,& M=0
\end{cases}
\]

On peut noter \[
X =M_X B
\] où \(M \sim Bern(q), \; ie\; P(M=1)=q\; \text{et}\; P(M=0)=1-q\)

\subsection{\texorpdfstring{Espérance de
\(X\)}{Espérance de X}}\label{esperance-de-x-1}

\[
\text{E}[X]=q\text{E}[B]
\]

\subsection{\texorpdfstring{Variance de
\(X\)}{Variance de X}}\label{variance-de-x-1}

\begin{align*}
Var(X)& = Var(M)\text{E}[B]^2+\text{E}[M]Var(B)\\
& =q(1-q)\text{E}[B]^2+qVar(B)
\end{align*}

\subsection{Fonction de répartition}\label{fonction-de-repartition-1}

\begin{align*}
F_X(x)& =P(M=0)+P(M=1)F_B(x),\;x\geq 0\\
& =1-q+q F_B(x)
\end{align*}

\subsection{\texorpdfstring{TLS de \(X\)}{TLS de X}}\label{tls-de-x-1}

\begin{align*}
\mathcal{L}_X(t)& = P_M(\mathcal{L}_B(t)),\; \text{où}\; P_M(S)=1-q+qS,\; S\in (0,1)\\
& =1-q+q\mathcal{L}_B(t),\;t \geq 0
\end{align*}

\subsection{VaR}\label{var}

Soit \(B\) tel que \(F_B(0)=0\). Alors, \[
F_X(0)=1-q+ qF_B(0)=1-q=P(M=0)=P(X=0)
\] On fixe \(\kappa \in (0,F_X(0)] \in (0,1-q]\) \[
\text{VaR}_\kappa (X) =F_X^{-1}(\kappa)=0
\]

On suppose que \(B\) est continue.\\
On fixe \(\kappa \in (F_X(0),1)\). Alors,

\[
F_X^{-1}(u), u \in (F_X(0),1)
\] est la solution de \[
F_X(x) =1-q+qF_B(x)=u
\] On déduit \[
F_B(x) =\dfrac{u-(1-q)}{q}
\] On obtient \[
F_X^{-1}(u) =F_B^{-1} \left( \dfrac{u-(1-q)}{q} \right),\;u\in(\underbrace{1-q}_{F_X(0)},1)
\] Puisque \(u\in(1-q,1)\Rightarrow \dfrac{u-(1-q)}{q}\in(0,1)\) et
\(F_X^{-1}{\dfrac{u-(1-q)}{q}}\) existe.\\
Ainsi, \[
\text{VaR}_\kappa (X) = 
\begin{cases}
0,& 0<u\leq 1-q\\
\text{VaR}_{\left({\dfrac{u-(1-q)}{q}}\right)} (B),& 1-q<u<1
\end{cases}
\]
\includegraphics{01-Introduction_files/figure-latex/unnamed-chunk-1-1.pdf}

\subsection{Esprance tronquée}\label{esprance-tronquee}

\[
\text{E}\left [X\times 1_{\{X>b\}}\right ]=P(M=1)\text{E}\left [B\times 1_{\{B>b\}}\right ],\; \text{pour}\;b\geq 0
\]

\subsection{TVaR}\label{tvar}

\[
\text{TVaR}_\kappa (X)=\dfrac{1}{1-\kappa}\text{E}\left [X\times 1_{\{X>\text{VaR}_\kappa (X) \}}\right ]+\dfrac{1}{1-\kappa}\text{VaR}_\kappa (X) \left (F_X(\text{VaR}_\kappa (X) )-\kappa\right )
\] On suppose \(F_B(0)=0\). \[
\text{Pour la partie:}\; \text{VaR}_\kappa (X) \left (F_X(\text{VaR}_\kappa (X) )-\kappa\right )\begin{cases}
\text{VaR}_\kappa (X) =0,& \text{si}\;\kappa \in(0,F_X(0)]\\
F_X(\text{VaR}_\kappa (X) )=\kappa,& \text{si}\;\kappa\in(F_X(0),1)
\end{cases}
\] Donc, dans les deux cas, \[
\dfrac{1}{1-\kappa}\text{VaR}_\kappa (X) \left (F_X(\text{VaR}_\kappa (X) )-\kappa\right )=0
\]

On suppose que \(B\) est continue.

\begin{align*}
\text{TVaR}_\kappa (X)& =\dfrac{1}{1-\kappa}\text{E}\left [X\times 1_{\{X>\text{VaR}_\kappa (X) \}}\right ]\\
& = \dfrac{1}{1-\kappa} P(M=1)\text{E}\left [B\times 1_{\{B>\text{VaR}_\kappa (X) \}}\right ]
\end{align*}

\section{Lois de fréquence}\label{lois-de-frequence}

La loi de Poisson est fondamentale en actuariat. On l`utilise pour
modeliser le nombre de sinistres pour le contrat.

\subsubsection*{\texorpdfstring{Caractéristiques
\(M \sim Pois(\lambda)\)}{Caractéristiques M \textbackslash{}sim Pois(\textbackslash{}lambda)}}\label{caracteristiques-m-sim-poislambda}
\addcontentsline{toc}{subsubsection}{Caractéristiques
\(M \sim Pois(\lambda)\)}

\begin{enumerate}
\item $\text{E}[M]=\lambda$
\item $Var(M)=\lambda$
\end{enumerate}

\subsection{\texorpdfstring{Loi de \(X\): loi Poisson
composée}{Loi de X: loi Poisson composée}}\label{loi-de-x-loi-poisson-composee}

\subsubsection{\texorpdfstring{Fgp de
\(M\)}{Fgp de M}}\label{fgp-de-m-1}

\[P_M(S)=e^{\lambda(S-1)},\;s \in (0,1)\]

\subsubsection{\texorpdfstring{TLS de
\(X\)}{TLS de X}}\label{tls-de-x-2}

\[\mathcal{L}_X(t)=P_M\left (\mathcal{L}_B(t)\right )= e^{\lambda\left (\mathcal{L}_B(t)-1\right )}\]

\subsubsection{\texorpdfstring{Espérance de
\(X\)}{Espérance de X}}\label{esperance-de-x-2}

Supposons \(\text{E}[B]<\infty\) \[
\text{E}[X]=\lambda\text{E}[B]
\]

\subsubsection{\texorpdfstring{Variance de
\(X\)}{Variance de X}}\label{variance-de-x-2}

Supposons \(Var(B)<\infty\)\\
Alors,

\begin{align*}
Var(X) &= \lambda\text{E}[B]^2+\lambda Var(B)\\
& =\lambda\text{E}[B^2]
\end{align*}

\subsection{Loi de Tweedie}\label{loi-de-tweedie}

Est utilisée dans la tarification en assurance dommages.\\
Soit \(B \sim Gamma(\alpha,\beta)\)

\begin{align*}
F_X(x)& = P(M=0)+\sum^\infty_{k=1} P(M=k) H(x;\alpha k,\beta),\;x \geq 0\\
& = e^{-\lambda} +\sum^\infty_{k=1} \dfrac{e^{-\lambda}\lambda^k}{k!}H(x;\alpha k,\beta) 
\end{align*}

Note:\\
En actuariat, une loi de fréquence où \(Var(M)=\text{E}[M]\) pose un
problème, car cette propriété n'est pas toujours observé en pratique. On
observe plutôt \(Var(X) \geq \text{E}[M]\).

\subsection{Loi binomiale négative}\label{loi-binomiale-negative}

Elle présente une alternative à la loi de Poisson en IARD. Elle est un
loi ``Poisson mélange''.\\
Caractéristiques:

\begin{enumerate}
\item $M \sim BinNeg(r,q),\;r \in (0,\infty),\; q\in (0,1)$
\item Loi de $X \sim BinNeg(r,q,F_B)$
\item $\text{E}[M]=\dfrac{r(1-q)}{q}$
\item $Var(M)=\dfrac{r(1-q)}{q^2}=\dfrac{\text{E}[M]}{q} \geq \text{E}[M],\; \text{pour}\; q\in(0,1)$
\end{enumerate}

\subsection{Loi Poisson mélange}\label{loi-poisson-melange}

La famille de lois Poisson mélange est importante en assurances de
dommages. Elles sont utilisées pour modéliser le nombre, ou le montant,
de sinistres pour un portefeuille hétérogène de risque.

Soit \(\Theta\) une V.A. de mélange tel que\\
\[\text{E}[\Theta]=1\] On suppose que
\(M|\Theta=\theta \sim Pois(\theta \lambda)\), une loi Poisson mélange.

\subsubsection{\texorpdfstring{Espérance de
\(M\)}{Espérance de M}}\label{esperance-de-m}

\begin{align*}
\text{E}[M]& =\text{E}_{\Theta}\left [\text{E}[M|\Theta]\right ]\\
& =\text{E}[\Theta \lambda] = \lambda\text{E}[\Theta]=\lambda
\end{align*}

Car, \(\text{E}[M|\Theta]=\theta\lambda\)

\subsubsection{\texorpdfstring{Variance de
\(M\)}{Variance de M}}\label{variance-de-m}

\(Var(\Theta)<\infty\)

\begin{align*}
Var(M)& = \text{E}_{\Theta}\left [Var(M|\Theta)\right ]+Var\left (\text{E}[M|\Theta]\right )\\
& =\text{E}[\lambda\Theta] + Var(\lambda\Theta)\\
& =\lambda\text{E}[\Theta]+\lambda^2 Var(\Theta)\\
& =\lambda +\lambda^2 Var(\Theta)=\lambda(1+Var(\Theta))>\lambda=\text{E}[M]
\end{align*}

Le V.A. \(\Theta\) représente ainsi l'incertitude liée aux
caractéristiques cachées des assurés. Cette variable permet de tenir
compte de l'hétérogénéité souvent présente dans un portefeuille de
contrat d'assurance de dommages.

\subsubsection{\texorpdfstring{Fgp de
\(M\)}{Fgp de M}}\label{fgp-de-m-2}

On suppose que la fgm de \(\Theta\) existe.

\begin{align*}
P_M(S)& =\text{E}[S^M]\\
& =\text{E}_{\Theta}\big [\underbrace{\text{E}[S^M|\Theta]}_{P_{M|\Theta}(S)}\big ]\\
& =\text{E}[e^{\lambda \Theta(S-1)}]\\
& =M_\Theta\left (\lambda(S-1)\right )
\end{align*}

Si \(\Theta\) est une V.A. discrète: \[
P(\Theta=\theta_j)=\alpha_j,\;j=1,2,\dots,\;\text{et}\;0<\theta_1<\dots<\theta_k
\]\\
Ainsi,

\begin{align*}
P(M=k)& = \sum^\infty_{j=1} P(\Theta=\theta_j)P(M=k|\Theta=\theta_j)\\
& = \sum^\infty_{j=1} \alpha_j \dfrac{e^{-\lambda \theta_j}(\lambda\theta_j)^k}{k!}  
\end{align*}

Si \(\Theta\) est une V.A. continue strictement positive:

\begin{align*}
P(M=k)& = \int^\infty_0 P(M=k|\Theta=\theta)f_\Theta(\theta)d\theta\\
& =\int_0^\infty e^{-\lambda \theta} \dfrac{(\lambda\theta)^k}{k!}f_\Theta(\theta)d\theta
\end{align*}

Si \(\Theta \sim Gamma(\alpha=r,\beta=r)\), \[
\text{E}[\Theta]=\dfrac{\alpha}{\beta}=\dfrac{r}{r}=1
\]\\
On veut identifier la loi de \(M\) à partir de sa fgp

\begin{align*}
P_M(S)& =M_\Theta\left (\lambda(S-1)\right )\\
& =\left (\dfrac{\beta}{\beta-\lambda(S-1)}\right )^\alpha\\
& =\left (\dfrac{r}{r-\lambda(S-1)}\right )^r
\end{align*}

Cela ressemble à une fgp d'une loi binomiale négative.\\
On sait que \[
\text{E}[M]=\dfrac{r(1-q)}{q}=\dfrac{r}{r}\lambda
\] Donc, \[
\dfrac{\lambda}{r}=\dfrac{(1-q)}{q}
\] Ainsi,

\begin{align*}
P_M(S)& =\left (\dfrac{1}{1-\dfrac{\lambda}{r}(S-1)}\right )^r\\
& =\left (\dfrac{1}{1-\dfrac{(1-q)}{q}(S-1)}\right )^r\\
& =\left (\dfrac{q}{q-(1-q)(S-1)}\right )^r\\
& =\left (\dfrac{q}{1-(1-q)S}\right )^r\\
& =\text{la fgp d'une BinNeg}
\end{align*}

\[
M \sim BinNeg(r,q),\;\text{où}\;r \in \Re^+\;\text{et}\; q \in (0,1)
\]

\chapter{Stats}\label{stats}

\section{Définitions}\label{definitions}

Observation: réalisation d'une variable aléatoire

Échantillon aléatoire de F: ensemble de V.A. iid

Statistiques: fonction d'un échantillon aléatoire et de constantes
connues

Paramètres: quantité d'intérêt(\(E[X],\;Var(x),\;etc\)) ou le paramètre
\(\theta\) d'un modèle paramétrique.

Statistique exhaustive: statistique qui contient toute l'information
pertinente sur le paramètre visé.

Estimateur: Statistique \(S(X_1,\dots,X_n)\) qui prend des valeurs qu'on
espère proche de \(\theta\) noté \(\hat{\theta}_n\)(Variable aléatoire)

Estimation de \(\theta\): données observées \(x_1,x_2,\dots\) de la
valeur observée \(\hat{\theta}\), \(s(x_1,x_2,\dots)\)(réalisations)

\section{Moyenne échantilonnale:}\label{moyenne-echantilonnale}

\[
\bar{X}_n=\frac{1}{n}\sum_{i=1}^n x_i
\]

\section{Variance échantillonale:}\label{variance-echantillonale}

\[
S^2_n= \frac{1}{n-1}\sum^n_{i=1}\left(X_i-\bar{X}_n\right)^2
\]

\section{Loi faible des grands
nombres:}\label{loi-faible-des-grands-nombres}

Soit \(X_1,X_2,...,\) une suite de V.A. iid. On suppose
\(var(X_i)< \infty\) et \(E[X] = \mu\), lorsque \(n \to \infty\)

\[ 
P\left(|\left(\bar{X}_n-\mu\right)|>\epsilon\right)\to 0\quad \forall\epsilon>0
\] \(\bar{X}_n\) converge en probabilité vers \(\mu\) \[
\bar{X}_n\overset{p}\to\mu\\  
\] Preuve par Tchebycheff: \[
P\left(|\bar{X}_n-\mu|>\epsilon\right)\leq\frac{var(X_i)}{n\epsilon^2}
\]

\section{Statistiques d'ordre d'un
échantillon:}\label{statistiques-dordre-dun-echantillon}

\begin{gather*}
    X_{(1)}=\min(X_1,\dots, X_n) \\  
    F_{X_{(1)}} (x)= 1 -{(1- F_X (x))}^n \\  
    \\
    X_{(n)}= \max(X_1,\dots,X_n) \\  
    F_{X_{(n)}}(x)={F_X(x)}^n\\
    \\
    f_{X_{(k)}}(x)= \frac{{n!}}{{(k-1)!}1{(n-k)!}}{F_X(x)}^{k-1}{(1-F_X(x))}^{n-k}f_X(x)
\end{gather*}

\section{\texorpdfstring{Distribution de
\(\bar{X}\):}{Distribution de \textbackslash{}bar\{X\}:}}\label{distribution-de-barx}

Soit \(X_1,\dots,X_n\), un échantillon de \(N(\mu,\frac{\sigma^2}{n})\),

\begin{gather*}
    \bar{X}_n=\frac{1}{n}\sum_{i=1}^n X_i\, \sim N\left(\mu,\frac{\sigma^2}{n}\right)\\
    \\
    Z_n=\frac{\bar{X}-\mu}{\frac{\sigma}{\sqrt{n}}}\,\sim N(0,1)
\end{gather*}

Utilisation de la distribution d'échantillonage de \(\bar{X}_n\):

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Vérifier une affirmation
\item
  Trouver un interval plausibe
\item
  Déterminer une taille d'échantillon minimal
\end{enumerate}

\section{Somme de normales au carré}\label{somme-de-normales-au-carre}

Soit \(Z_1,\dots,Z_n\sim N(0,1)\) \[
\sum_{i=1}^n Z_i^2\sim \chi^2_n
\]

Soit \(X_1,\dots,X_n\sim N(\mu,\sigma^2)\) \[
\frac{(n-1)S^2_n}{\sigma^2}= \frac{1}{\sigma^2}\sum^n_{i=1}(X_i-\bar{X}_n)^2\sim \chi^2_{(n-1)}
\] \(S^2_n\bot\,\bar{X}_n\) \[
E[S^2_n]= \frac{\sigma^2}{(n-1)}E\left[\frac{(n-1)}{\sigma^2}S^2_n\right]=\frac{\sigma^2}{(n-1)}(n-1)=\sigma^2
\]

\section{Statistique Student}\label{stats:stats:student}

\[
\text{T}_n= \sqrt{n}\frac{\bar{X}_n-\mu}{\sqrt{S_n^2}}
\]

\section{Distribution de la Statistique
Student}\label{stats:dist:student}

\begin{align*}
    \text{T}_n& =\sqrt{n} \frac{\bar{X}_n -\mu}{\sqrt{S^2_n}}\sim t(n-1)\\
    \text{T}_n& =\frac{\bar{X}_n-\mu}{\sqrt{\frac{\sigma^2}{n}}}\times\sqrt{\frac{\sigma^2}{S^2_n}}\\
    & =\underbrace{\frac{\bar{X}_n-\mu}{\sqrt{\frac{\sigma^2}{n}}}}_{\sim N(0,1)}\times\underbrace{\sqrt{\frac{(n-1)}{(n-1)\frac{S^2_n}{\sigma^2}}}}_{\sim \chi_{(n-1)}^2}
\end{align*}

\section{Distribution Student}\label{distribution-student}

Soit \(Z\sim N(0,1)\) et \(W\sim \chi^2_{(v)}\) \(Z\bot W\) \[
T=\frac{Z}{\sqrt{\frac{W}{n}}}\sim t(v)
\] Propriété

Si \(v>1\): \(E[T]=0\)

Si \(v>2\): \(Var(T)=\frac{v}{v-2}\)\\
Si \(v\rightarrow \infty,\;t(v)\;\text{converge vers}\;N(0,1)\)

\section{Statistique F}\label{stats:stats:f}

Soit \(X_i,\dots,X_n\sim N(\mu_1,\sigma^2_1)\) et
\(Y_1,\dots,Y_n \sim N(\mu_2,\sigma_2^2)\)\\
Pour comparer: \(\sigma_1^2\) et \(\sigma^2_2\) \[
\frac{S_n^2}{S_m^2}=\frac{\frac{1}{n-1}\sum_{i=1}^n(X_i-\bar{X}_n)^2}{\frac{1}{m-1}\sum_{i=1}^m(Y_i-\bar{Y}_m)^2}
\]

\section{Distribution F}\label{stats:dist:f}

Soit \(W_1\sim\chi^2_{(v_1)},\;W_2\sim\chi^2_{(v_2)}\)

\(W_1 \bot\, W_2\)

\[
F=\dfrac{W_1}{v_1}\div\dfrac{W_2}{v_2}\quad
F\sim F(v_1,v_2)
\]

Si \(X \sim F(v_1,v_2)\) et \(v_2>2\)
\(E\left[X =\frac{v_2}{v_2-2}\right]\)

\section{Comparer variance
échantionnale}\label{stats:stats:f:varuxe9chan}

Soit \(X_1,\dots,X_n\sim N(\mu_1,\sigma^2_1)\) et
\(Y_1,\dots,Y_m\sim N(\mu_2,\sigma^2_2)\) \[
\dfrac{S^2_n}{\sigma^2_1}\div\dfrac{S_m^2}{\sigma^2_2}\sim F(n-1,m-1)
\]

\section{Lemme de Slutsky}\label{lemme-de-slutsky}

Soit \(X_1,X_2,\dots\) et \(Y_1,Y_2,\dots\) Lorsque
\(n \rightarrow \infty\) et \(X_n \rightsquigarrow X\) et
\(Y_n \rightsquigarrow c\)

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \(X_n+Y_n \rightsquigarrow X+c\)
\item
  \(X_n\times Y_n \rightsquigarrow X\times c\)
\item
  Si \(c>0,\; \frac{X_n}{Y_n} \rightsquigarrow \frac{X}{c}\)
\end{enumerate}

\section{Théorème Central Limite}\label{stats:tcl}

\BeginKnitrBlock{theorem}
\protect\hypertarget{thm:unnamed-chunk-1}{}{\label{thm:unnamed-chunk-1}
}Soit \(X_1,\dots,X_n\), un échantillon d'une V.A. quelconque: \[
Z_n=\sqrt{n}\times\frac{\bar{X}_n-\mu}{\sigma}
\] quand \(n \to \infty\): \[
P(Z_n\le X)\to\phi(X)
\] Convergence en distribution: \[
Z_n\rightsquigarrow N(0,1)
\]
\EndKnitrBlock{theorem}

\section{Distribution Statistique Student lorsqu'on ne connait pas la
variance et X ne provient pas d'une loi
Normale}\label{distribution-statistique-student-lorsquon-ne-connait-pas-la-variance-et-x-ne-provient-pas-dune-loi-normale}

\(T \not\sim t(n-1)\)\\
Soit \(X_1,\dots,X_n\), un échantillon d'une V.A quelconque:\\
\(E[X^4]<\infty\), lorsque \(n\to\infty\): \[
T_n = \sqrt{n}\frac{\bar{X}_n-\mu}{S_n}\rightsquigarrow N(0,1)
\]

\section{Aproximation de la loi Binomiale avec la loi
Normale}\label{aproximation-de-la-loi-binomiale-avec-la-loi-normale}

\[
Z=\sqrt{n}\times \frac{\bar{X}_n-p}{\sqrt{p(1-p)}}\approx N(0,1)
\] Correction de la continuité: \[
P(Y\le y)\approx P(Z \le y+0.5 )
\]

\section{Critères pour évaluer la performance d'un
estimateur}\label{stats:criteres}

\subsection{Critère 1: Biais}\label{stats:criteres:biais}

\begin{gather*}
B(\hat{\theta}_n)= E\left[\hat{\theta}_n-\theta\right]=E\left[\hat{\theta}_n\right]-\theta\\
\intertext{Estimateur sans biais:}\\
B(\hat{\theta}_n)=0\\
\intertext{Estimateur asymptotiquement sans biais:}\\
\lim_{n\to \infty} B(\hat{\theta}_n)=0
\end{gather*}

Voir preuve \ref{preuves:biais:xn} et \ref{preuves:biais:sn} pour le
développement des biais de \(\bar{X}_n\) et \(S_n^2\)

\subsection{Critère 2: Variance}\label{stats:criteres:variance}

Parmi 2 estimateurs sans biais, on préfère celui avec une plus petite
variance.

Pour deux estimateur avec biais, on préfère celui avec la plus petite
Erreur quadratique moyenne(EQM): \[
EQM\left(\hat{\theta}_n\right)= E\left[(\hat{\theta}_n-\theta)^2\right]= Var\left(\hat{\theta}_n\right)+\left[B(\hat{\theta}_n)\right]^2
\]

\subsection{Critère 3: Convergence}\label{stats:convergence}

L'estimateur \(\hat{\theta}_n\) est un estimateur convergent de
\(\theta\) si, quand \(n\to \infty\), \[
\hat{\theta}_n\xrightarrow{P}\theta
\] ce qui signifie que pour tout \(\epsilon>0\), \[
\lim_{n\to\infty}P(|\hat{\theta}-\theta|>\epsilon)=0
\]

Voir \ref{preuves:convergence} pour prouver la convergence

\section{Efficacité relative}\label{stats:effrela}

Soit \(\hat{\theta}_n\) et \(\tilde{\theta}_n\), 2 estimateurs sans
biais et convergent.

\[
\text{eff}(\hat{\theta}_n,\tilde{\theta}_n)= \frac{Var(\tilde{\theta}_n)}{Var(\hat{\theta}_n)}
\] Si \(\text{eff}(\hat{\theta}_n,\tilde{\theta}_n)>1\),
\(\hat{\theta}_n\) est préférable, sinon \(\tilde{\theta}_n\) est
préférable.

\section{Définition formelle statistique
exhaustive}\label{definition-formelle-statistique-exhaustive}

Une statistique exhaustive est une statistique qui contient toute
l'information pertinente sur le paramètre visé.

Soit \(X_1,\dots,X_n\) un échantillon aléatoire d'une distribution avec
paramètre \(\theta\) inconnu. Alors, la statistique \[
T=T(X_1,\dots,X_n)
\] est exhaustive pour \(\theta\) ssi la distribution conditionnelle de
\(X_1,\dots,X_n\) sachant \(T\) ne dépend pas de \(\theta\).

\section{Théoreme de factorisation de
Fischer-Neyman}\label{theoreme-de-factorisation-de-fischer-neyman}

Soit \(X_1,\dots,X_n\) un échantillon aléatoire d'une distribution avec
densité \(f(\cdot;\theta)\) et paramètre \(\theta\) inconnu. Alors, la
statistique \[
T=T(X_1,\dots,X_n)
\] est exhaustive pour \(\theta\) ssi,
\(\forall\;x_1,\dots,x_n\;\in\mathbb{R}\), \[
f(x_1;\theta)\times\dots\times f(x_n;\theta)=g(t;\theta)\times h(x_1,\dots,x_n)
\] où

\begin{itemize}
\tightlist
\item
  \(g(t;\theta)\) ne dépend de \(x_1,\dots,x_n\) qu'à travers \(t\).
\item
  \(h(x_1,\dots,x_n)\) ne dépend pas de \(\theta\).
\end{itemize}

Avec plus d'un paramètre:

Soit \(X_1,\dots,X_n\) un échantillon aléatoire d'une distribution avec
densité \(f(\cdot;\theta)\) et paramètres
\(\theta=\theta_1,\dots,\theta_n\) inconnus. Alors, les statistiques \[
T_1=T_1(X_1,\dots,X_n),\dots,T_k=T_k(X_1,\dots,X_n)
\] sont conjointements exhaustives pour \(\theta\) ssi,
\(\forall\;x_1,\dots,x_n\;\in\mathbb{R}\), \[
f(x_1;\theta)\times\dots\times f(x_n;\theta)=g(t_1,\dots,t_k;\theta)\times h(x_1,\dots,x_n)
\] où

\begin{itemize}
\tightlist
\item
  \(g(t_1,\dots,t_n;\theta)\) ne dépend de \(x_1,\dots,x_n\) qu'à
  travers \(t_1,\dots,t_k\).
\item
  \(h(x_1,\dots,x_n)\) ne dépend pas de \(\theta\).
\end{itemize}

\section{Critère de Lehmann-Scheffé}\label{critere-de-lehmann-scheffe}

Soit \(X_1,\dots,X_n\) un échantillon aléatoire d'une distribution avec
densité \(f(\cdot;\theta)\) et paramètre \(\theta\) inconnu. Alors, la
statistique \[
T=T(X_1,\dots,X_n)
\] est exhaustive minimale pour \(\theta\) ssi,
\(\forall\;x_1,\dots,x_n,y_1,\dots,y_n\;\in\mathbb{R}\), \[
\frac{f(x_1;\theta)\times\dots\times f(x_n;\theta)}{f(y_1;\theta)\times\dots\times f(y_n;\theta)}
\] ne dépend pas de \(\theta\) ssi \[
T(X_1,\dots,X_n)=T(Y_1,\dots,Y_n)
\]

\section{Théorème de Rao-Blackwell}\label{theoreme-de-rao-blackwell}

\label{stats:rao} \(\hat{\theta}_n\) un estimateur sans biais tel que
\(var(\hat{\theta}_n)<\infty\). Si \(T\) est exhaustive pour \(\theta\),
la statistique: \[
\theta^{*}_n=E[\hat{\theta}_n|T]
\] est un estimateur sans biais et \[
var(\theta^{*}_n)\le var(\hat{\theta}_n)
\] Voir \autoref{preuves:rao}.

\section{Estimateur sans biais et de variance
minimale(MVUE)}\label{estimateur-sans-biais-et-de-variance-minimalemvue}

Un estimateur \(\hat{\theta}_n\) est sans biais et de variance minimale
si:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \(\hat{\theta}_n\) est sans biais.
\item
  \(\hat{\theta}_n=g(T)\), où \(T\) est une statistique exhaustive
  (minimale) obtenue avec le théorème Fischer-Neymann.
\end{enumerate}

\subsection{Construire un MVUE}\label{construire-un-mvue}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Trouver une statistique exhaustive (minimale) \(T\) avec le théorème
  Fischer-Neymann.
\item
  Trouver une fonction \(g\) tel que: \(E[g(T)]=\theta\)
\item
  Poser \(\hat{\theta}_n=g(T)\).
\end{enumerate}

\section{Méthode des moments}\label{methode-des-moments}

Si \(t\) paramètres sont inconnus, on résout le système à \(t\)
équations: \[
m_k=E\left[X^k\right],\quad k=1,\dots,t
\] Les estimateurs obtenus sont appelés les estimateurs des moments.

\section{Méthode des quantiles}\label{methode-des-quantiles}

Pour certaine loi, les moments n'existent pas. Pour estimer \(t\)
paramètres inconnus, on pourrait résoudre le système à \(t\) équations:
\[
\hat{\pi}_{\kappa j}=VaR_{\kappa j}(X)\quad j=1,\dots,t
\]

\section{Quantile empirique lissé}\label{quantile-empirique-lisse}

Pour un échantillon aléatoire \(X_1,\dots,X_n\) le quantile empirique de
niveau \(\kappa\in(0,1)\) est: \[
\hat{\pi}_{\kappa,n}=(1-h)X_{(j)}+hX_{(j+1)}\quad j=\lfloor(n+1)\kappa\rfloor\;et \; h=(n+1)\kappa-j 
\]

\section{Fonction de vraissemblance}\label{fonction-de-vraissemblance}

Soit \(X_1,\dots,X_n\) un échantillon aléatoire d'une distribution avec
fmp ou fdd: \[
f(x;\theta),\quad \theta \in \Theta
\] où \(\Theta\) est l'ensemble des valeurs possibles du paramètre. Si
\(x_1,\dots,x_n\) sont des valeurs observées de l'échantillon, la
vraissemblance de \(\theta\) basée sur \(x_1,\dots,x_n\) est définie
comme: \[
L(\theta)=f(x_1;\theta)\times\dots\times f(x_n;\theta)
\]

\subsection{Observation}\label{observation}

\begin{itemize}
\tightlist
\item
  \(X\) est discrète: vraisemblance de \(\theta\) basée sur
  \(x_1,\dots,x_n\) est exactement la probabilité d'observer
  \(x_1,\dots,x_n\).
\item
  \(X\) est continue: vraisemblance de \(\theta\) basée sur
  \(x_1,\dots,x_n\) est la densité et est proportionelle à la
  probabilité d'observer \(x_1,\dots,x_n\).
\item
  la vraisemblance est vue comme une fonction réelle déterministe de
  \(\theta\)
\item
  La vraisemblance est l'objet dans le théorème de factorisation de
  Fischer-Neymann
\item
  la vraisemblance \(L(\theta)\) devrait être plus grande pour des
  valeurs de \(\theta\) proche de celle du mécanisme générateur de
  données.
\item
  on estime donc \(\theta\) par la valeur \(\hat{\theta}_n\) qui
  maximise \(L(\theta)\): \[
  \hat{\theta}_n=\underset{\theta\in\Theta}{arg\,max}\,L(\theta) 
  \]
\end{itemize}

\section{Estimateur du maximum de
vraissemblance}\label{estimateur-du-maximum-de-vraissemblance}

Soit \(x_1,\dots,x_n\), les valeurs observées d'un échantillon aléatoire
de \(f(x;\theta)\), où \(\theta=(\theta_1,\dots,\theta_k)\) sont des
paramètres inconnus (\(\in \Theta\), l'espace des paramètres). La valeur
observée de l'EMV de \(\theta\) est la valeur \(\hat{\theta}\) qui
maximise la vraisemblance de \(\theta\) basée sur \(x_1,\dots,x_n\) \[
\hat{\theta}_n=\underset{\theta\in\Theta}{argmax}\,L(\theta)
\] On suppose que le max est unique.

Les EMVs sont basés sur des statistiques exhaustives, sont souvent
biaisés, mais habituellement asymptotiquement sans biais. On obtient
souvent des MVUEs lorsqu'on corrige le biais.

\section{Log-vraisemblance}\label{log-vraisemblance}

\[
\ell(\theta)=ln\left (L(\theta)\right )=\sum^n_{i=1} ln\left (f(x_i;\theta)\right )
\] Cette fonction est strictement croissante. Ainsi, l'EMV peut être
obtenu en maximisant la log-vraisemblance. \[
\hat{\theta}=\underset{\theta\in\Theta}{argmax}\,\ell(\theta)
\]

\subsection{EMV et exhaustivité}\label{emv-et-exhaustivite}

Avec le théorème de Fischer-Neymann \[
L(\theta)=g(t,\theta)\,h(x_1,\dots,x_n)
\]\\
et la log-vraisemblance \[
\ell(\theta)=ln(g(t,\theta))+ln(h(x_1,\dots,x_n))
\]

Ainsi, puisque \(h(x_1,\dots,x_n)\) ne dépend pas de \(\theta\),
l'estimation du MV est \[
\underset{\theta\in\Theta}{argmax}\, ln(g(t,\theta))
\] une fonction de la valeur \(t\) de la statistique exhaustive \(T\).

\section{Propriété de l'EMV pour de grands
échantillons}\label{propriete-de-lemv-pour-de-grands-echantillons}

Sous certaines conditions de régularité sur la fonction de masse de
probabilté ou de la densité, l'EMV \(\hat{\theta}\) existe et est unique
avec probabilité convergeant vers 1 lorsque \(n\rightarrow \infty\) \[
\dfrac{\hat{\theta}_n-\theta}{\sqrt{\dfrac{1}{nI(\theta)}}}\leadsto N(0,1)
\] quand,\(n\rightarrow\infty\).

\subsubsection*{Information de Fischer}\label{information-de-fischer}
\addcontentsline{toc}{subsubsection}{Information de Fischer}

\[
I(\theta)=\text{E}\left [\left (\dfrac{\partial}{\partial \theta}ln f(X;\theta)\right )^2\right ]
\]

On peut aussi formuler l'EMV pour les grandes valeurs de n comme \[
\hat{\theta}\approx N\left(\theta,\dfrac{1}{nI(\theta)}\right)
\] et

\begin{align*}
I(\theta)& =\text{E}\left [\left (\dfrac{\partial}{\partial \theta}ln f(X;\theta)\right )^2\right ]\\
& =\text{E}\left [-\dfrac{{\partial}^2}{\partial {\theta}^2}ln f(X;\theta)\right ]
\end{align*}

Si les conditions de régularité sont vérifées, l'EMV est convergent \[
\hat{\theta}_n\leadsto \theta
\] et \[
\lim_{n\rightarrow\infty} var(\hat{\theta}_n)=0
\]

On peut montrer que tout estimateur sans biais \(\hat{\theta}\)
satisfait l'inégalité de Cramer-Rao: \[
var(\hat{\theta}) \geq \dfrac{1}{nI(\theta)}
\]

L'EMV est donc asymptotiquement efficace. Il a la plus petite variance
possible, asymptotiquement. On retrouve des formes fermées pour les EMVs
dans les cas simples seulement. On doit obtenir les autres par des
techniques d'optimisation numérique. La fonction de log-vraisemblance
est convexe et lisse, ce qui facilite l'optimisation. On utilise les
estimateurs des moments comme valeurs de départs dans les fonctions
d'optimisation.

\section{Propriété d'invariance de
l'EMV}\label{propriete-dinvariance-de-lemv}

Supposons que le paramètre d'intérèt est \(\lambda=g(\theta)\), où \(g\)
est une fonction bijective sur \(\Theta\). Puisque \[
L(\theta)=L(g^{-1}(\lambda))
\] le maximum est atteint à \[
\hat{\theta}_n=g^{-1}(\hat{\lambda}_n)\Leftrightarrow \hat{\lambda}_n=g(\hat{\theta}_n)
\] où \(\hat{\theta}_n\) est l'EMV de \(\theta\).

\section{Diagramme quantile-quantile}\label{diagramme-quantile-quantile}

Outil pour vérifier l'ajustement d'un modèle.

Si \(x_1,\dots,x_n\) sont les données d'un modèle paramétrique qu'on a
ajusté avec fonction de répartition \(F(\cdot;\theta)\). Le diagramme
Q-Q comprend les points \[
\left (F^{-1}\left(\dfrac{1}{n+1};\hat{\theta}_n\right),x_{(i)}\right ),\;i=1,\dots,n
\] où \(x_{(1)} \leq \dots \leq x_{(n)}\) sont les observations
ordonnées de \(x_1,\dots,x_n\).

Avec ce diagramme, on compare les quantiles empiriques avec les
quantiles théoriques. Si le modèle est bien ajusté, les points devraient
formés une droite. S'ils forment une courbe vers le haut/bas, les
quantiles théoriques sont trop petits/grands.

\section{Critère d'information
d'Akaike}\label{critere-dinformation-dakaike}

Soit un modèle paramétrique pour les données, avec paramètre \[
\theta=(\theta_1,\dots,\theta_k) 
\]\\
\(\hat{\theta}_n\) est l'EMV de \(\theta\). Le critère d'information
d'Akaike est \[
AIC = 2(-\ell(\hat{\theta}_n)+k)
\] On préfère le modèle avec le plus petit \(AIC\)

\section{Critère d'information bayésien de
Schwartz}\label{critere-dinformation-bayesien-de-schwartz}

\[
BIC = -2\ell(\hat{\theta}_n)+k ln(n)
\]

\chapter{GRF-2}\label{grf-2}

\section{Chapitre 1}\label{chapitre-1}

\subsection*{Produit dérivé}\label{produit-derive}
\addcontentsline{toc}{subsection}{Produit dérivé}

Contrat entre 2 partis qui fixe les flux monétaires futurs fondés sur
ceux de l'actif sous-jacent(SJ).

\subsection*{Étapes d'une transaction}\label{etapes-dune-transaction}
\addcontentsline{toc}{subsection}{Étapes d'une transaction}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Acheteur et vendeur se trouve. Facilité par la bourse.
\item
  Obligations pour les deux partis définis (prix, produits, conditions).
  Si transaction à la bourse: intermédiaire, et donc, dépôts de garantie
  possibles.
\item
  Transaction
\item
  Mise à jour du registre de propriété.
\end{enumerate}

\subsection*{Mesure d'évaluation taille(activité) de la
bourse(marché)}\label{mesure-devaluation-tailleactivite-de-la-boursemarche}
\addcontentsline{toc}{subsection}{Mesure d'évaluation taille(activité)
de la bourse(marché)}

\begin{itemize}
\tightlist
\item
  Volume de transaction: nombre de titres transigés parpériodes
\item
  Valeur marchande: Valeur d'une action/cie/indice boursier
\item
  Positions ouvertes: quantité de contrats qui ne sont pas arrivés à
  échéance
\end{itemize}

\subsection*{Rôle des marchés
financiers}\label{role-des-marches-financiers}
\addcontentsline{toc}{subsection}{Rôle des marchés financiers}

\begin{itemize}
\tightlist
\item
  Partage du risque: compagnie partage le risque et les profits avec les
  actionnaires
\item
  Diversification du risque: risque diversifiable \(\rightarrow\)
  théoriquement possible de diluer le risque pour qu'il devienne nul.
  Risque non-diversifiable \(\rightarrow\) possible de transférer le
  risque via des produits dérivés.
\end{itemize}

\subsection*{Utilité des produits
dérivés}\label{utilite-des-produits-derives}
\addcontentsline{toc}{subsection}{Utilité des produits dérivés}

\begin{itemize}
\tightlist
\item
  Gestion des risques
\item
  Spéculation
\item
  Réduction des frais de transaction
\item
  arbitrage réglementaire
\end{itemize}

\subsection*{3 types d'acteurs}\label{types-dacteurs}
\addcontentsline{toc}{subsection}{3 types d'acteurs}

\begin{itemize}
\tightlist
\item
  Uilisateurs(acheteur/vendeurs)
\item
  Teneur de marché(intermédiaire)
\item
  Observateur(analyste/autorité)
\end{itemize}

\subsection*{Définitions}\label{definitions-1}
\addcontentsline{toc}{subsection}{Définitions}

\begin{itemize}
\tightlist
\item
  Ordre au cours du marché: quantité de l'actif visé à acheter(vendre)
  au prix du marché, au moment où l'ordre est passée.
\item
  Ordre à cours limité: quantité d'actions à acheter/vendre dans une
  tranche spécifique de prix.
\item
  Ordre de vente «stop»: prix en dessous duquel on vend automatiquement.
\item
  Position longue: qui profitera de l'augmentation de la valeur du SJ.
\item
  Position courte: qui profitera de la diminution de la valeur du SJ.
\item
  Vente à découvert: vente d'un actif qu'on ne possède pas. L'actif est
  livré à une date ultérieur, mais paiement à t=0 au prix de l'actif à
  t=0.

  \begin{itemize}
  \tightlist
  \item
    Utilité:

    \begin{itemize}
    \tightlist
    \item
      Spéculation
    \item
      Financement
    \item
      Couverture contre la baisse de valeur
    \end{itemize}
  \item
    Risque:

    \begin{itemize}
    \tightlist
    \item
      de défaut
    \item
      de rareté
    \end{itemize}
  \end{itemize}
\end{itemize}

\subsection{CAPM(Capital asset pricing
management)}\label{capmcapital-asset-pricing-management}

\subsubsection*{3 postulats:}\label{postulats}
\addcontentsline{toc}{subsubsection}{3 postulats:}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Transactions efficaces et sans friction: pas de frais de transaction,
  emprunt au taux sans risque.
\item
  Rationnalité des investisseurs: maximise leur ratio de Sharpe \[
  \rightarrow\; \frac{E[R_p- r_f]}{\sigma_p}
  \]
\item
  Attentes et espérances homogènes
\end{enumerate}

L'équation du CAPM pour un actif \(\mathrm{i}\):

\[
R_i =r_f+a_i+\beta_i(R_{mkt}-r_f)+\epsilon\\  
\] Cela implique:\\
\[
\frac{dR_i}{d R_{mkt}}=\beta_i=\frac{Cov(R_i,R_{mkt})}{Var(R_{mkt})}\\    
\] Pour un portefeuille p:\\
\[
\frac{dR_p}{d R_{mkt}}=\beta_p=\frac{Cov(\sum x_i R_i,R_{mkt})}{Var(R_{mkt})}=\sum x_i \frac{Cov(R_i,R_{mkt})}{Var(R_{mkt})}=\sum x_i \beta_i
\]

\subsubsection*{Incohérences du modèle}\label{incoherences-du-modele}
\addcontentsline{toc}{subsubsection}{Incohérences du modèle}

\begin{itemize}
\tightlist
\item
  Investisseurs non rationnels et pas informés sur leur portefeuille
\item
  Certains ne veulent pas nécessairement maximiser leur ratio de Sharpe,
  ont d'autres objectifs
\item
  Il y a des investisseurs qui ne diversifient pas leur portefeuille de
  manière optimale
\item
  Il y en a qui sont ultra-actif, malgré le fait que le CAPM suppose une
  gestion passive
\end{itemize}

Comportements avec effet plus systémique:

\begin{itemize}
\tightlist
\item
  Peur du regret: garder un titre qui est en train de baisser ou vendre
  un titre avant qu'il remonte
\item
  Les investisseurs sont influençables; ils achèteront les titres
  médiatisés,etc.
\item
  Effet de trouppeau: on fait comme ceux qu'on connait
\end{itemize}

\subsection{Modèle multifactoriel et l'APT(arbitrage pricing
theory)}\label{modele-multifactoriel-et-laptarbitrage-pricing-theory}

Trois types d'actifs avec des alphas strictement positifs qui
contredisent le CAPM:

\begin{itemize}
\tightlist
\item
  Petites capitalisations: on observe des rendements supérieurs à ce que
  le CAPM prédit
\item
  Book to market ratio: titres ``value'' avec une valeur au livre
  supérieur à la valeur marchande verront la valeur marchande rejoindre
  la valeur au livre avec le temps
\item
  Momemtum: les compagnies qui ont connues un bon rendement dernièrement
  auront tendance à avoir un rendement supérieur à la moyenne
\end{itemize}

\subsubsection{APT}\label{apt}

\[
E[R_s]-r_f= \sum_{i=1}^N \beta_s^{Fi}(E[R_{Fi}]-r_f)
\] Les ``F'' sont des facteurs. Il est possible de créer des modèles
avec n'importe quels facteurs comme des indices boursiers.

\section{Chapitre 2}\label{chapitre-2}

\subsection{Contrat Foward}\label{contrat-foward}

Achat d'un actif prédeterminé à une valeur initiale \(S_0\), à une date
de livraison \(T\) et à un prix \(F_{0,T}\). Le coût initial est nul.
\(F_{0,T}\) est le prix anticipé de l'acftif sous-jacent rendu à la date
\(T\). \(S_0(1+r_f)^T=F_{0,T}\)

\begin{itemize}
\tightlist
\item
  Valeur à l'échéance:

  \begin{itemize}
  \tightlist
  \item
    Pour l'acheteur(position longue): \(F_{0,T} - S_T\)
  \item
    Pour le vendeur(position courte): \(S_t - F_{0,T}\)
  \end{itemize}
\end{itemize}

\includegraphics{03-GRF-2_files/figure-latex/unnamed-chunk-1-1.pdf}

\subsection{Foward prépayé}\label{foward-prepaye}

Dans certain cas, l'acheteur voudra payé à \(t=0\). Le coût initial sera
\(F_{0,T}^P\). On achète immédiatemment sans avoir l'actif à la date de
transaction. La position de l'acheteur est \emph{capitalisée}. Dans un
achat ferme, la position de l'acheteur est pleinement capitalisée. Le
contrat foward, lui, implique une position non capitalisée.

\begin{tabularx}{0.5\textwidth}{Xrr}
\toprule
Temps & Acheteur & Vendeur \\
\midrule
$t=0$ & $-F_{0,T}^P$ & $F_{0,T}^P$ \\
$t=T$ & $S_T$ & $-S_T$ \\
\bottomrule
\end{tabularx}

\includegraphics{03-GRF-2_files/figure-latex/unnamed-chunk-3-1.pdf}

Pour recréer les cashflows d'un contrat foward avec un achat ferme, on
finance l'achat ferme avec un emprunt au taux sans risque.

\begin{tabular}{lrrr}
\toprule
Temps & Achat ferme & + Emprunt & = Foward \\
\midrule
$t=0$ & $-S_0$ & $S_0$ & $\varnothing$ \\
$t=T$ & $S_T$ & $F_{0,T}$ & $S_T - F_{0,T}$ \\
\bottomrule
\end{tabular}

On peut aussi recréer les cashflows d'un achat ferme avec un foward et
en investissant la valeur actualisée de \(F_{0,T}\).\\
\(F_{0,T}(1+r_f)^{-T}=S_0(1+r_f)^{T}(1+r_f)^{-T}=S_0\).

\begin{tabular}{lrrr}
\toprule
Temps & Dépot & + Foward & = Achat ferme \\
\midrule
$t=0$ & $-S_0$ & $\varnothing$ & $-S_0$ \\
$t=T$ & $F_{0,T}$ & $S_T - F_{0,T}$ & $S_T$ \\
\bottomrule
\end{tabular}

\subsection{Option d'achat(call)}\label{option-dachatcall}

Contrat qui permet au détenteur(position longue) d'acheter un actif
sous-jacent à un prix prédéterminé, strike price \(=K\), à une date
d'échéance ou d'içi cette date, s'il le désire.

3 types de levées:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Européenne (à la date T)
\item
  Américaine (d'içi la date T)
\item
  Bermudienne (à certains moments d'içi T)
\end{enumerate}

\begin{tabular}{lrr}
\toprule
\multicolumn{3}{c}{Profit} \\
\midrule
Actif SJ & Acheteur & Vendeur \\
\midrule
$S_T > K$ & $S_T - K - C(K,T)(1+r_f)^T$ & $K-S_T+C(K,T)(1+r_f)^T$\\
$S_T<K$ & $- C(K,T)(1+r_f)^T$ & $C(K,T)(1+r_f)^T$\\
\bottomrule
\end{tabular}

\includegraphics{03-GRF-2_files/figure-latex/unnamed-chunk-4-1.pdf}

\begin{tabularx}{0.5\textwidth}{Xr}
\toprule
\multicolumn{2}{c}{Valeur à l'échéance}\\
\midrule
Acheteur & $max(0;S_T-K)$\\
Vendeur & $-max(0;S_T-K)$\\
\bottomrule
\end{tabularx}

\subsection{Option de vente(put)}\label{option-de-venteput}

Contrat qui permet au détenteur(position courte) de vendre un actif
sous-jacent à un prix prédéterminé, strike price \(=K\), à une date
d'échéance ou d'içi cette date, s'il le désire. Le vendeur(position
longue) de l'option devra acheter le SJ à ce prix si le
détenteur(acheteur) le désire.

\begin{tabular}{ccc}
\toprule
\multicolumn{3}{c}{Option de vente} \\  
\midrule
Position & Profit & Valeur à l'échéance \\
\midrule
Acheteur & $max(0;K-S_T)-P(K,T)(1+r_f)^T$ & $max(0;K-S_T)$ \\
\\
Vendeur & $P(1+r_f)^T-max(0;K-S_T)$ & $-max(0;K-S_T)$\\
\bottomrule
\end{tabular}

\includegraphics{03-GRF-2_files/figure-latex/unnamed-chunk-5-1.pdf}

\section{Floor}\label{floor}

Combinaison d'une postion longue dans le SJ(on le possède) et une
position courte dans une option de vente(achat). Permet de se couvrir
contre une baisse du prix du SJ.

\begin{tabular}{|c|c|}
\hline 
\rule[-1ex]{0pt}{2.5ex} Valeur à l'échéance & $=S_T+\max(0;K-S_T)=\max(S_T;K)$ \\ 
\hline 
\rule[-1ex]{0pt}{2.5ex} Profit & $=\max(S_T,K)-(S_0+P(T,K))(1+r_f)^T$ \\ 
\hline 
\end{tabular}

\section{Vente de couverture:vendre un floor(option de vente
couverte)}\label{vente-de-couverturevendre-un-flooroption-de-vente-couverte}

Combinaison d'une position longue dans l'option de vente(vente) et d'une
position courte dans le SJ(vente à découvert)

\begin{tabular}{|c|c|}
\hline 
\rule[-1ex]{0pt}{2.5ex} Valeur à l'échéance & $=-S_T-\max(0;K-S_T)=-\max(S_T;K))$ \\ 
\hline 
\rule[-1ex]{0pt}{2.5ex} Profit & $=-\max(S_T,K)+(S_0+P(T,K))(1+r_f)^T$ \\ 
\hline 
\end{tabular}

\section{Cap}\label{cap}

Combinaison d'une position courte dans le SJ(vente à découvert)et d'une
position longue dans une option d'achat(achat).

\begin{tabular}{|c|c|}
\hline 
\rule[-1ex]{0pt}{2.5ex} Valeur à l'échéance & $=S_T+\max(0;S_T-K)=\max(-S_T;-K)=-\min(S_T;K)$ \\ 
\hline 
\rule[-1ex]{0pt}{2.5ex} Profit & $=-\min(S_T,K)+(S_0-C(T,K))(1+r_f)^T$ \\ 
\hline 
\end{tabular}

\section{Vente de couverture:vendre un cap(option d'achat
couverte)}\label{vente-de-couverturevendre-un-capoption-dachat-couverte}

Combinaison d'une position courte dans l'option d'achat(vente) et d'une
position longue dans le SJ(achat).

\begin{tabular}{|c|c|}
\hline 
\rule[-1ex]{0pt}{2.5ex} Valeur à l'échéance & $=S_T-\max(0;S_T-K)=-\max(-S_T;-K)=\min(S_T;K)$ \\ 
\hline 
\rule[-1ex]{0pt}{2.5ex} Profit & $=\min(S_T,K)-(S_0-C(T,K))(1+r_f)^T$ \\ 
\hline 
\end{tabular}

\section{Foward synthétique}\label{foward-synthetique}

On fait un foward en combinant une position longue dans une option
d'achat et une position longue dans une option de vente avec la même
échéance et le même strike price.

\begin{tabular}{|c|c|}
\hline 
Foward synthétique & $=Call(K,T)-Put(K,T)$ \\ 
\hline 
Coût initial & $C(K,T)-P(K,T)$ \\ 
\hline 
Valeur à l'échéance & $=\max(0;S_T-K)-\max(0;K-S_T)=S_T-K$ \\ 
\hline 
Profit & $(S_T-K)-(C(K,T)-P(K,T))(1+r_f)^T$ \\ 
\hline 
\end{tabular}

Si on remplace \(K\) par \(F_{0,T}\), le prix d'exercice sera le même
qu'avec un foward standard. La différence avec un foward synthétique est
que \(K\not=F_{0,T}\) est possible et, ainsi, le coût initial ne sera
pas nul. Si \(K<F_{0,T}\), on payera le SJ moins cher, mais on payeune
prime initiale. Si \(K>F_{0,T}\), on payera plus cher le SJ, mais on
recevra une prime initiale.

\section{Parité des options d'achat et de
vente}\label{parite-des-options-dachat-et-de-vente}

\begin{align*}
E\left [S_T-K-(C(K,T)-P(K,T)(1+r_f)^T\right ]& =E\left [\text{Profit}\right ]=0\\
E\left [S_T\right ]-K& =(C(K,T)-P(K,T))(1+r_f)^T\\
C(K,T)-P(K,T)& = (F_{0,T}-k)(1+r_f)^{-T}
\end{align*}

\section{Bull spread}\label{bull-spread}

\subsection{Première façon:}\label{premiere-facon}

Combinaison d'une position longue dans une option d'achat à un prix
d'exercice \(K_1\) et d'une position courte dans une option d'achcat à
un prix d'exercice \(K_2,\;K_1<K_2\), avec la même date d'échéance.

\begin{tabular}{|c|c|}
\hline 
Bull spread(call)& $=Call(K_2,T)-Call(K_1,T)$ \\ 
\hline 
Coût initial & $C(K_1,T)-C(K_2,T)$ \\ 
\hline 
Valeur à l'échéance & $=\max(0;S_T-K_1)-\max(0;S_T-K_2)$ \\ 
\hline 
Profit & $\max(0;S_T-K_1)-\max(0;S_T-K_2)-(C(K_1,T)-C(K_2,T))(1+r_f)^T$ \\ 
\hline 
\end{tabular}

\subsection{Deuxième façon:}\label{deuxieme-facon}

Combinaison d'une position courte(achat) dans une option de vente à un
prix d'exercice \(K_1\) et d'une position longue(vente) dans une option
de vente à un prix d'exercice \(K_2,\;K_1<K_2\), avec la même date
d'échéance.

\begin{tabular}{|c|c|}
\hline 
Bull spread(Put)& $=Put(K_2,T)-Put(K_1,T)$ \\ 
\hline 
Coût initial & $P(K_1,T)-P(K_2,T)$ \\ 
\hline 
Valeur à l'échéance & $=\max(0;K_1-S_T)-\max(0;K_2-S_T)$ \\ 
\hline 
Profit & $\max(0;K_1-S_T)-\max(0;K_2-S_T)-(P(K_1,T)-P(K_2,T))(1+r_f)^T$ \\ 
\hline 
\end{tabular}

\section{Bear Spread(-Bull spread)}\label{bear-spread-bull-spread}

\subsection{Première façon:}\label{premiere-facon-1}

Combinaison d'une position courte(vente) dans une option d'achat à un
prix d'exercice \(K_1\) et d'une position longue(achat) dans une option
d'achcat à un prix d'exercice \(K_2,\;K_1<K_2\), avec la même date
d'échéance.

\begin{tabular}{|c|c|}
\hline 
Bear spread(call)& $=Call(K_1,T)-Call(K_2,T)$ \\ 
\hline 
Coût initial & $C(K_2,T)-C(K_1,T)$ \\ 
\hline 
Valeur à l'échéance & $=\max(0;S_T-K_2)-\max(0;S_T-K_1)$ \\ 
\hline 
Profit & $\max(0;S_T-K_2)-\max(0;S_T-K_1)-(C(K_2,T)-C(K_1,T))(1+r_f)^T$ \\ 
\hline 
\end{tabular}

\subsection{Deuxième façon:}\label{deuxieme-facon-1}

Combinaison d'une position longue(vente) dans une option de vente à un
prix d'exercice \(K_1\) et d'une position courte(achat) dans une option
de vente à un prix d'exercice \(K_2,\;K_1<K_2\), avec la même date
d'échéance.

\begin{tabular}{|c|c|}
\hline 
Bull spread(Put)& $=Put(K_1,T)-Put(K_2,T)$ \\ 
\hline 
Coût initial & $P(K_2,T)-P(K_1,T)$ \\ 
\hline 
Valeur à l'échéance & $=\max(0;K_2-S_T)-\max(0;K_1-S_T)$ \\ 
\hline 
Profit & $\max(0;K_2-S_T)-\max(0;K_1-S_T)-(P(K_2,T)-P(K_1,T))(1+r_f)^T$ \\ 
\hline 
\end{tabular}

\section{Ratio spread}\label{ratio-spread}

Combinaison de n position longue(achat) dans les options d'achat à un
prix d'exercice \(K_1\) et m position courte(vente) dans les options
d'achat à un prix d'exercice \(K_2\), avec la m\^{}me date d'échéance.
Permet la possibilité de créer une combinaison quirésulte en un coût
initial nul.

\section{Box spread}\label{box-spread}

Combinaison de positions longue dans une option d'achat(achat) et de
vente(vente) à un prix d'exercice \(K_1\) et de positions courtes dans
une option d'achat(vente) et d'une option de vente(achat) à un prix
d'exercice \(K_2\), avec toutes les options de mêmes dates d'échéance.

\begin{tabular}{|c|c|c|}
    \hline 
    Box spread &  $=Call(K_1,T)+Put(K_2,T)$ & $-Call(K_2,T)-Put(K_1,T)$ \\ 
    \hline 
    Box spread & Bull spread(call) & + Bear spread(put) \\ 
    \hline 
    Box spread & $=Call(K_1,T)-Call(K_2,T)$ & $+Put(K_2,T)-Put(K_1,T)$ \\ 
    \hline 
    Box spread & Foward synthétique $K_1$ & - Foward synthétique $K_2$  \\ 
    \hline 
    Box spread & $=Call(K_1,T)-Put(K_1,T)$ & $-(Call(K_2,T)-Put(K_2,T))$ \\ 
    \hline 
    Coût initial & $=C(K_1,T)+P(K_2,T)$ & $-C(K_2,T)-P(K_1,T)>0$ \\ 
    \hline 
    Valeur à l'échéance & $=\max(0;S_T-K_1)+\max(0;K_2-S_T)$ & $-\max(0;S_t-K_2)-\max(0;K_1-S_T)$ \\ 
    \hline 
    Profit & \multicolumn{2}{c|}{=0} \\
    \hline
\end{tabular}

\chapter{Preuves}\label{preuves}

\section{Théorème (\ref{thm:fn-quantile}) de la fonction
quantile}\label{theoreme-refthmfn-quantile-de-la-fonction-quantile}

\label{preuves:fn-quantile} \BeginKnitrBlock{proof}

\iffalse{} {Proof. } \fi{}

\begin{align*}
F_{F_X^{-1}(u)}(x)& =P(F_X^{-1} \le x)\\
& =P(U \le F_X(x))\\
& =F_X(x)
\end{align*}
\EndKnitrBlock{proof}

\section{Fonction
Stop-Loss(\ref{intro:fn-stop})}\label{fonction-stop-lossrefintrofn-stop}

\label{preuves:fn-stop} \BeginKnitrBlock{proof}

\iffalse{} {Proof. } \fi{}

\begin{align*}
\Pi_X(d)& = E[\max(X-d, 0)]\\
& =E\left[X \times \mathrm{1}_{ \{X > d\} } -d\times \mathrm{1}_{\{X > d\}}\right]\\
& =E\left[X \times \mathrm{1}_{\{X > d\}}\right]-d \bar{F}(d)\\
\end{align*}
\EndKnitrBlock{proof}

\section{Tvar}\label{tvar-1}

\subsection{Expresion alternative
1(\ref{intro:tvar:alt1})}\label{expresion-alternative-1refintrotvaralt1}

\label{preuves:tvar:1} \BeginKnitrBlock{proof}

\iffalse{} {Proof. } \fi{}On applique \ref{intro:fn-quantile:1}, ainsi:

\begin{align*}
\text{TVaR}_k(X)& =\frac{1}{(1-k)}\int_k^1\text{VaR}_k(u)\,du\\
& =\frac{1}{1-k}\left(\Pi_X(\text{VaR}_k(X))\right)+\text{VaR}_k(X)
\end{align*}
\EndKnitrBlock{proof}

\subsection{Expression alternative
2(\ref{intro:tvar:alt2})}\label{expression-alternative-2refintrotvaralt2}

\label{preuves:tvar:2} \BeginKnitrBlock{proof}

\iffalse{} {Proof. } \fi{}On remplace \(\Pi_X(\text{VaR}_k(X))\) dans
\ref{preuves:tvar:2} par sa définition \ref{intro:fn-stop}

\begin{align*}
\text{TVaR}_k(X)& = \text{VaR}_k(X)+\frac{1}{(1-k)}\left(E[X\times\mathrm{1}_{\{X>\text{VaR}_k(X)\}}]-\text{VaR}_k(X)\bar{F}_X(\text{VaR}_k(X))\right)\\
& =\frac{1}{(1-k)}\left[E\left[X\times\mathrm{1}_{\{X>\text{VaR}_k(X)\}}\right]-\text{Var}_k(X)\left(\bar{F}_X(\text{VaR}_k(X))-(1-k)\right)\right]\\
& =\frac{1}{(1-k)}\left[E\left[X\times\mathrm{1}_{\{X>\text{VaR}_k(X)\}}\right]-\text{Var}_k(X)\left(F_X(\text{VaR}_k(X))-k\right)\right]
\end{align*}
\EndKnitrBlock{proof} Pour une V.A. continue
\(\text{VaR}_k(X)\left(F_X(\text{VaR}_k(X))-k\right)=0\) donc,
\[\text{TVaR}_k(X)= \frac{E\left[X\times\mathrm{1}_{\{X>\text{VaR}_k(X)\}}\right]}{P(X>\text{VaR}_k(X))}= E\left[X|X>\text{VaR}_k(X)\right]\]

\subsection{Expression alternative
3(\ref{intro:tvar:alt3})}\label{expression-alternative-3refintrotvaralt3}

\label{preuves:tvar:3} On fait la preuve à partir de l'expression
alternative 2: \BeginKnitrBlock{proof}

\iffalse{} {Proof. } \fi{}

\begin{align*}
\text{TVaR}_k(X)& =\frac{1}{(1-k)}\left[E[X\times\mathrm{1}_{\{X>\text{VaR}_k(X)\}}]-\text{VaR}_k(X)(F_X(\text{VaR}_k(X))-k)\right]\\
& =\frac{1}{(1-k)}\left[E[X\times\mathrm{1}_{\{X>\text{VaR}_k(X)\}}+X\times\mathrm{1}_{\{X=\text{VaR}_k(X)\}}-X\times\mathrm{1}_{\{X=\text{VaR}_k(X)\}}\right]\\
& -\text{VaR}_k(X)\left(1-\bar{F}_X(\text{VaR}_k(X))-(1-(1-k))\right)\\
& =\frac{1}{(1-k)}\left\{E[X\times\mathrm{1}_{\{X\ge\text{VaR}_k(X)\}}]-E[X\times\mathrm{1}_{\{X=\text{VaR}_k(X)\}}]+\text{VaR}_k(X)\left[(1-k)-P(X>\text{VaR}_k(X))\right]\right\}\\
& =\frac{1}{(1-k)}\left\{E[X\times\mathrm{1}_{\{X\ge\text{VaR}_k(X)\}}]-(E[X\times\mathrm{1}_{\{X=\text{VaR}_k(X)\}}]+P(X>\text{VaR}_k(X))\times\text{VaR}_k(X))\right\}
\end{align*}

Deux cas possibles:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  V.A. discrète \(P(X=\text{VaR}_k(X))>0\)
\item
  V.A. continue \(P(X=\text{VaR}_k(X))=0\)
\end{enumerate}

Donc la portion
\((E[X\times\mathrm{1}_{\{X=\text{VaR}_k(X)\}}]+P(X>\text{VaR}_k(X))\times\text{VaR}_k(X))= \text{VaR}_k(X)[1-\frac{P(X\ge\text{VaR}_k(X))}{(1-k)}]\)
\EndKnitrBlock{proof}

\subsection*{Propriété}\label{propriete-2}
\addcontentsline{toc}{subsection}{Propriété}

\subsubsection*{Sous-additivité}\label{sous-additivite-1}
\addcontentsline{toc}{subsubsection}{Sous-additivité}

\label{preuves:tvar:prop:sousadd} 3 preuves. La première est basée sur
les statistiques d'ordre, la deuxième est basée sur la représentation de
la \(\text{TVaR}\) par la stop-loss.\\
1ere preuve: \BeginKnitrBlock{proof}

\iffalse{} {Proof. } \fi{}1er lemme: Soit une V.A. \(X\) quelconque,
dont \(E[X]<\infty\).\\
Soit m réalisations indépendantes de \(X\): \(X^{(1)},\dots,X^{(m)}\).

\[
\text{TVaR}_{\kappa}(X) =\frac{\lim_{m\to\infty} \left(\sum^n_{j=\lfloor m\kappa\rfloor +1} X^{[j]} \right)}{\lfloor m(1-\kappa)\rfloor},\;\text{pour}\;\lfloor m\kappa \rfloor <m
\]\\
Où,\\

\begin{align*}
\lfloor x \rfloor& =\text{partie entière de}\;x\\
X^{[1]}\le X^{[2]}\le \dots \le X^{[m]}& =\text{réalisations triées de} X
\end{align*}

2e lemme:\\
Soit les réalisations : \(X^{(1)},\dots,X^{(m)}\)\\
On définit \(X^{[1]}\le X^{[2]} \le \dots \le X^{[m]}\) comme les
réalisations triées de \(X\).\\

\begin{align*}
\sum^m_{j=m-1}X^{[j]}& = \sup\{X^{(j_1)}+X^{(j_2)},\; 1\le j_1 \le j_2 \le m\}\\
\sum^m_{j=m-2}X^{[j]}& = \sup\{X^{(j_1)}+X^{(j_2)}+X^{(j_3)},\; 1\le j_1 \le j_2 \le j_3 \le m\}\\
\sum^m_{j=k_0+1}X^{[j]}& = \sup\{X^{(j_1)}+X^{(j_2)}+\dots+X^{(j_{m-k_0})},\; 1\le j_1 \le j_2 \le \dots \le j_{m-k_0} \le m\}
\end{align*}

Soit les V.A. \(X_1,X_2\) avec \(E[X_i]<\infty,\;i=1,2\).\\
\(S=X_1+X_2\)

Avec le 1er lemme:\\
\[\text{TVaR}_{\kappa}(S) =\frac{\lim_{m\to\infty} \left(\sum^n_{j=\lfloor m\kappa\rfloor +1} S^{[j]} \right)}{\lfloor m(1-\kappa)\rfloor}\]

On développe \(\sum^m_{j=\lfloor m\kappa\rfloor +1} S^{[j]}\) en
utilisant le 2e lemme et on pose \(\kappa_0=\lfloor m\kappa \rfloor\)

\begin{align*}
\sum^m_{ j =  \lfloor m \kappa \rfloor +1 } S^{ [j] } & = \sup \{ S^{ (j_1) } + \dots + S^{ ( j_{ m-\lfloor m \kappa \rfloor } ) } ,1 \le j_1 \le \dots \le j_{ m- \lfloor m \kappa \rfloor } \le m \} \\
& = \sup \{ \left( X^{ (j_1) }_1 + X^{ (j_1) }_2 \right) +\left( X^{ (j_2) }_1 + X^{ (j_2) }_2 \right) + \dots + \left( X^{ ( j_{ m - \kappa_0 } ) }_1 + X^{ ( j_{ m-\kappa_0} ) }_2 \right) \\ 
& ,1 \le j_1 \le \dots \le j_{ m-\kappa_0 }\le m \} \\
& = \sup \{ \left( X_1^{ (j_1) } + X_1^{ (j_2) } + \dots + X_1^{ ( j_{ m-\kappa_0 } ) } \right) +\left( X_2^{ (j_1) } + X_2^{ (j_2) } + \dots + X_2^{ ( j_{ m-\kappa_0 } ) } \right) \\
& ,1 \le j_1 \le \dots \le j_{ m-\kappa_0 } \le m \}\\ 
& \le \sup \{ \left( X_1^{ (j_1) } + X_1^{ (j_2) } + \dots + X_1^{ ( j_{ m-\kappa_0 } ) } \right) ,1 \le j_1 \le  \dots \le j_{ m-\kappa_0 } \le m \}\\
& + \sup \{ \left( X_2^{ (j_1) } + X_2^{ (j_2) } + \dots +X_2^{ ( j_{ m-\kappa_0 } ) } \right) ,1 \le j_1 \le \dots \le j_{ m -\kappa_0 } \le m \} \\
& = \sum^m_{ j=\kappa_0 +1 } X_1^{ [j] } + \sum^m_{ j=\kappa_0 +1 } X_2^{ [j] }
\end{align*}

On applique le 1er lemme de chaque coté de l'inégalité\\
\(\sum^m_{j=\kappa_0 +1} S^{[j]}\le\sum^m_{j=\kappa_0 +1} X_1^{[j]}+\sum^m_{j=\kappa_0 +1} X_2^{[j]}\)

\begin{align*}
\text{TVaR}_\kappa(S)& = \lim_{m\to\infty}\frac{1}{\lfloor m(1-\kappa)\rfloor} \sum^m_{j=\kappa_0 +1} S^{[j]}\\
& \le \lim_{m\to\infty}\frac{1}{\lfloor m(1-\kappa)\rfloor} \sum^m_{j=\kappa_0 +1} X_1^{[j]} + \lim_{m\to\infty}\frac{1}{\lfloor m(1-\kappa)\rfloor} \sum^m_{j=\kappa_0 +1} X_2^{[j]}\\
& =\text{TVaR}_\kappa(X_1)+\text{TVaR}_\kappa(X_2)
\end{align*}
\EndKnitrBlock{proof} 2e preuve:

\begin{align*}
\text{TVaR}_\kappa(X)& =\text{VaR}_\kappa+\frac{1}{1-\kappa}\Pi_X(VaR_\kappa(X))\\
& =\phi(VaR_\kappa(X))\\
\text{où}\;\phi(X)& =x + \frac{1}{1-\kappa}\Pi_X(x)\\
\text{et}\; \Pi_X(x)& = E\left[\max(X-x;0)\right]
\end{align*}

Donc, \[
\text{TVaR}_\kappa(X)= \inf \phi(X),\; \text{où}\;\phi(X) \text{ est une fonction convexe}   
\\
\text{ le minimum est atteint à}\; \text{VaR}_\kappa(X)
\] \includegraphics{04-Preuves_files/figure-latex/unnamed-chunk-7-1.pdf}
Vérification que \(\phi(X)\) ext convexe en \(x\):\\
Supposons que \(X\)est continue:

\begin{align*}
\phi(X)& =x + \frac{1}{1-\kappa} \int^{\infty}_x \bar{F}_X(y)dy,\;x\geq 0\\
\\
\text{Dérivée première de }\phi(X):\\
\frac{d\phi(X)}{dx}& =1+\frac{1}{1-\kappa}(-\bar{F}_X(x))\\
\\
\text{Dérivée seconde de }\phi(X):\\
\frac{d^2\phi(X)}{d^2x}& =\frac{1}{1-\kappa}f_X(x)\geq 0,\quad x\geq 0
\end{align*}

Valeur qui minimise \(\phi(X)\):

\begin{align*}
\frac{d\phi(X)}{dx}& = 1+\frac{1}{1-\kappa}(-\bar{F}_X(x))=0\\
\bar{F}_X(x)& =1-\kappa\\
F_X(x)& =\kappa
\end{align*}

Alors,

\begin{align*}
\text{TVaR}_\kappa(X)& =\phi_X(\text{VaR}_\kappa(X)) \leq \phi_X(x),\quad x\in \mathrm{R}
\end{align*}

Soit \(X_1\) et \(X_2\) tel que \(E[X_i]\le\infty\), pour \(i=1,2\)\\
\(S=X_1+X_2\), \(\kappa\in(0,1)\).\\
On développe \(\text{TVaR}_\kappa((1-\alpha)X_1+\alpha X_2)\), où
\(\alpha\in(0,1)\)

\begin{align*}
\text{TVaR}_\kappa((1-\alpha)X_1+\alpha X_2)& = \phi_{((1-\alpha)X_1+\alpha X_2)}(\text{VaR}_\kappa((1-\alpha)X_1+\alpha X_2))\\
& \leq x\frac{1}{1-\kappa}\Pi_{((1-\alpha)X_1+\alpha X_2)}(x),\quad x\in\mathrm{R}\\
& =x+\frac{1}{1-\kappa}E\left[ \max\left((1-\alpha)X_1+\alpha X_2;0\right)\right],\quad x\in\mathrm{R}\\
\text{On fixe }x=(1-\alpha)\text{VaR}_\kappa(X_1)+\alpha \text{VaR}_\kappa(X_2)\\
\\
\text{TVaR}_\kappa((1-\alpha)X_1+\alpha X_2)& \leq (1-\alpha)\text{VaR}_\kappa(X_1)+\alpha \text{VaR}_\kappa(X_2)\\
& +\frac{1}{1-\kappa}E\left[ \max((1-\alpha)X_1+\alpha X_2-(1-\alpha)\text{VaR}_\kappa(X_1)-\alpha \text{VaR}_\kappa(X_2);0)\right],\\
&\;\text{vrai pour }\alpha\in(0,1)\\
& =(1-\alpha)\text{VaR}_\kappa(X_1)+\alpha \text{VaR}_\kappa(X_2)\\
& +\frac{1}{1-\kappa}E\left[ \max((1-\alpha)(X_1-\text{VaR}_\kappa(X_1))\alpha (X_2-\text{VaR}_\kappa(X_2));0)\right]\\
& \leq (1-\alpha)\text{VaR}_\kappa(X_1)+\alpha \text{VaR}_\kappa(X_2)\\
& +\frac{1}{1-\kappa}E\left[\max((1-\alpha)(X_1-\text{VaR}_\kappa(X_1));0)\right]\\
& +\frac{1}{1-\kappa}E\left[\max((\alpha)(X_2-\text{VaR}_\kappa(X_2));0)\right]\\
& = \text{VaR}_\kappa((1-\alpha)X_1)+\text{VaR}_\kappa(\alpha X_2)\\
& +\frac{1}{1-\kappa}E\left[\max((1-\alpha)X_1-\text{VaR}_\kappa((1-\alpha)X_1))\right]\\
& +\frac{1}{1-\kappa}E\left[\max(\alpha X_2-\text{VaR}_\kappa(\alpha X_2)\right]\\
& =\text{TVaR}_\kappa((1-\alpha)X_1)+\text{TVaR}_\kappa(\alpha X_2),\quad \alpha\in(0,1)\\
\text{On fixe }\alpha=\frac{1}{2}\\
\text{TVaR}_\kappa(\frac{1}{2} X_1+\frac{1}{2} X_2)& =\text{TVaR}_\kappa(\frac{1}{2}(X_1+X_2))\\
& =\frac{1}{2}\text(TVaR)_\kappa(X_1+X_2)\\
& \leq \frac{1}{2}\text(TVaR)_\kappa(X_1)+\frac{1}{2}\text(TVaR)_\kappa(X_2)\\
\text{On multiplie par 2 et on déduit}:\\
\text(TVaR)_\kappa(X_1+X_2)& \leq \text(TVaR)_\kappa(X_1)+\text(TVaR)_\kappa(X_2)
\end{align*}

\section{Biais moyenne échantillonale (voir
\ref{stats:criteres:biais})}\label{biais-moyenne-echantillonale-voir-refstatscriteresbiais}

\label{preuves:biais:xn} \BeginKnitrBlock{proof}

\iffalse{} {Proof. } \fi{}

\begin{align*}
B(\hat{\theta}_n)& =E[\bar{X}_n]-E[X]\\
& =E[X]-E[X]=0
\end{align*}
\EndKnitrBlock{proof}

\section{Biais variance échantillonale (voir
\ref{stats:criteres:biais})}\label{biais-variance-echantillonale-voir-refstatscriteresbiais}

\label{preuves:biais:sn} \BeginKnitrBlock{proof}

\iffalse{} {Proof. } \fi{}

\begin{align*}
S_n^2& =\frac{1}{n-1}\left(\sum_{i=1}^n {(X_i-\bar{X}_n)}^2\right)\\
& =\frac{1}{n-1}\left(\sum_{i=1}^n (X_i^2-2X_i\bar{X}_n+\bar{X}_n^2)\right)\\
& =\frac{1}{n-1}\left[\left(\sum_{i=1}^n X_i^2\right)  -\frac{2}{n-1}\left(\bar{X}_n\sum_{i=1}^n X_i\right) +\frac{n}{(n-1)}\bar{X}_n^2\right]\\
& =\frac{1}{n-1}\left(\sum_{i=1}^n X_i^2\right) - \frac{n}{(n-1)}\bar{X}_n^2
\end{align*}

\begin{align*}
E\left[S_n^2\right]& =E\left[\frac{1}{n-1}\left(\sum_{i=1}^n X_i^2\right)\right]-E\left[\frac{n}{(n-1)}(\bar{X}_n)\right]\\
& =\frac{n}{n-1}((Var(X)+E^2[X])) - \frac{1}{(n-1)}(Var(X))-\frac{n}{n-1}(E[X^2])\\
& =Var(X)
\end{align*}

\begin{gather*}
B(S^2_n)= Var(X)-\sigma^2 = 0
\end{gather*}
\EndKnitrBlock{proof}

\section{Convergence (voir
\ref{stats:convergence})}\label{convergence-voir-refstatsconvergence}

\label{preuves:convergence} \BeginKnitrBlock{proof}

\iffalse{} {Proof. } \fi{}On prouve avec Tchebycheff\\
Un estimateur sans biais est convergent si: \[
\lim_{n\to \infty} Var(\hat{\theta}_n) =0
\]

\begin{gather*}
\text{On fixe}\; \epsilon>0,\\
P(|\hat{\theta}_n-\theta|>\epsilon)= P(|\hat{\theta}_n-E[\hat{\theta}_n]|>\epsilon)\\
=P(|\hat{\theta}_n-E[\hat{\theta}_n]|>\frac{\epsilon\times \sqrt{Var(\hat{\theta}_n)}}{\sqrt{Var(\hat{\theta}_n)}})\\
\le \frac{Var(\hat{\theta}_n)}{\epsilon^2}
\end{gather*}

Donc si \(Var(\hat{\theta}_n)\to 0\) quand \(n \to \infty\),
\(\hat{\theta}_n\) est convergent
\EndKnitrBlock{proof}

\section{\texorpdfstring{Téorème de Rao-Blackwell (voir
\autoref{stats:rao})}{Téorème de Rao-Blackwell (voir )}}\label{teoreme-de-rao-blackwell-voir}

\label{preuves:rao} Puisque \(T\) est exhaustive pour \(\theta\), la
distribution conditionnelle de \((X_1,\dots,X_n)\) sachant \(T\) ne
dépend pas de \(\theta\). Alors, \[
\theta^{*}_n=E[\hat{\theta}_n|T]
\] ne dépend pas de \(\theta\). Donc, \(\theta^{*}_n\) est une
statistique. Par l'espérance totale, \[
E[\theta^{*}_n]=E[E[\hat{\theta}_n|T]]=E[\hat{\theta}_n]=\theta,
\] \(\theta^{*}_n\) est donc sans biais. Par la variance totale,

\begin{align*}
var(\hat{\theta}_n) &=var(E[\hat{\theta}_n|T])+E[var(\hat{\theta}_n|T)] \\
&= var(\theta^{*}_n)+E[var(\hat{\theta}_n|T)]\\
\end{align*}

Sachant que \[
E[var(\hat{\theta}_n|T)] \ge 0 
\] \[
var(\theta^{*}_n)\le var(\hat{\theta}_n)
\]

\bibliography{book.bib}


\end{document}
