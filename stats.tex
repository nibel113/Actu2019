\chapter{Stats}\label{stats}


\section{Définitions}\label{definitions}

\begin{enumerate}
\item Observation: réalisation d'une variable aléatoire
\item Échantillon aléatoire de F: ensemble de V.A. iid
\item Statistiques: fonction d'un échantillon aléatoire et de constantes connues
\item Paramètres: quantité d'intérêt(\(E[X],\;Var(x),\;etc\)) ou le paramètre \(\theta\) d'un modèle paramétrique.
\item Statistique exhaustive: statistique qui contient toute l'information pertinente sur le paramètre visé.
\item Estimateur: Statistique \(S(X_1,\dots,X_n)\) qui prend des valeurs qu'on espère proche de \(\theta\) noté \(\hat{\theta}_n\)(Variable aléatoire)
\item Estimation de \(\theta\): données observées \(x_1,x_2,\dots\) de la valeur observée \(\hat{\theta}\), \(s(x_1,x_2,\dots)\)(réalisations)
\end{enumerate}

\section{Moyenne échantilonnale:}\label{moyenne-echantilonnale}

\[
\bar{X}_n=\frac{1}{n}\sum_{i=1}^n x_i
\]

\section{Variance échantillonale:}\label{variance-echantillonale}

\[
S^2_n= \frac{1}{n-1}\sum^n_{i=1}\left(X_i-\bar{X}_n\right)^2
\]

\section{Loi faible des grands nombres:}\label{loi-faible-des-grands-nombres}

Soit \(X_1,X_2,...,\) une suite de V.A. iid. On suppose
\(var(X_i)< \infty\) et \(E[X] = \mu\), lorsque \(n \to \infty\)

\[ 
P\left(|\left(\bar{X}_n-\mu\right)|>\epsilon\right)\to 0\quad \forall\epsilon>0
\] \(\bar{X}_n\) converge en probabilité vers \(\mu\) \[
\bar{X}_n\overset{p}\to\mu\\  
\] Preuve par Tchebycheff: \[
P\left(|\bar{X}_n-\mu|>\epsilon\right)\leq\frac{var(X_i)}{n\epsilon^2}
\]

\section{Statistiques d'ordre d'un échantillon:}\label{statistiques-dordre-dun-echantillon}

\begin{gather*}
    X_{(1)}=\min(X_1,\dots, X_n) \\  
    F_{X_{(1)}} (x)= 1 -{(1- F_X (x))}^n \\  
    \\
    X_{(n)}= \max(X_1,\dots,X_n) \\  
    F_{X_{(n)}}(x)={F_X(x)}^n\\
    \\
    f_{X_{(k)}}(x)= \frac{{n!}}{{(k-1)!}1{(n-k)!}}{F_X(x)}^{k-1}{(1-F_X(x))}^{n-k}f_X(x)
\end{gather*}

\section{Distribution de \(\bar{X}\):}\label{distribution-de-barx}

Soit \(X_1,\dots,X_n\), un échantillon de \(N(\mu,\frac{\sigma^2}{n})\),

\begin{gather*}
    \bar{X}_n=\frac{1}{n}\sum_{i=1}^n X_i\, \sim N\left(\mu,\frac{\sigma^2}{n}\right)\\
    \\
    Z_n=\frac{\bar{X}-\mu}{\frac{\sigma}{\sqrt{n}}}\,\sim N(0,1)
\end{gather*}

Utilisation de la distribution d'échantillonage de \(\bar{X}_n\):

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Vérifier une affirmation
\item
  Trouver un interval plausibe
\item
  Déterminer une taille d'échantillon minimal
\end{enumerate}

\section{Somme de normales au carré}\label{somme-de-normales-au-carre}

Soit \(Z_1,\dots,Z_n\sim N(0,1)\) \[
\sum_{i=1}^n Z_i^2\sim \chi^2_n
\]

Soit \(X_1,\dots,X_n\sim N(\mu,\sigma^2)\) \[
\frac{(n-1)S^2_n}{\sigma^2}= \frac{1}{\sigma^2}\sum^n_{i=1}(X_i-\bar{X}_n)^2\sim \chi^2_{(n-1)}
\] \(S^2_n\bot\,\bar{X}_n\) \[
E[S^2_n]= \frac{\sigma^2}{(n-1)}E\left[\frac{(n-1)}{\sigma^2}S^2_n\right]=\frac{\sigma^2}{(n-1)}(n-1)=\sigma^2
\]

\section{Statistique Student}\label{stats:stats:student}

\[
\text{T}_n= \sqrt{n}\frac{\bar{X}_n-\mu}{\sqrt{S_n^2}}
\]

\section{Distribution de la Statistique Student}\label{stats:dist:student}

\begin{align*}
    \text{T}_n& =\sqrt{n} \frac{\bar{X}_n -\mu}{\sqrt{S^2_n}}\sim t(n-1)\\
    \text{T}_n& =\frac{\bar{X}_n-\mu}{\sqrt{\frac{\sigma^2}{n}}}\times\sqrt{\frac{\sigma^2}{S^2_n}}\\
    & =\underbrace{\frac{\bar{X}_n-\mu}{\sqrt{\frac{\sigma^2}{n}}}}_{\sim N(0,1)}\times\underbrace{\sqrt{\frac{(n-1)}{(n-1)\frac{S^2_n}{\sigma^2}}}}_{\sim \chi_{(n-1)}^2}
\end{align*}

\section{Distribution Student}\label{distribution-student}

Soit \(Z\sim N(0,1)\) et \(W\sim \chi^2_{(v)}\) \(Z\bot W\) \[
T=\frac{Z}{\sqrt{\frac{W}{n}}}\sim t(v)
\] 
\subsection*{Propriétés}

Si \(v>1\): \(E[T]=0\)\\
Si \(v>2\): \(Var(T)=\frac{v}{v-2}\)\\
Si \(v\rightarrow \infty,\;t(v)\;\text{converge vers}\;N(0,1)\)

\section{Statistique F}\label{stats:stats:f}

Soit \(X_i,\dots,X_n\sim N(\mu_1,\sigma^2_1)\) et
\(Y_1,\dots,Y_n \sim N(\mu_2,\sigma_2^2)\)\\
Pour comparer: \(\sigma_1^2\) et \(\sigma^2_2\) \[
\frac{S_n^2}{S_m^2}=\frac{\frac{1}{n-1}\sum_{i=1}^n(X_i-\bar{X}_n)^2}{\frac{1}{m-1}\sum_{i=1}^m(Y_i-\bar{Y}_m)^2}
\]

\section{Distribution F}\label{stats:dist:f}

Soit \(W_1\sim\chi^2_{(v_1)},\;W_2\sim\chi^2_{(v_2)}\)

\(W_1 \bot\, W_2\)

\[
F=\dfrac{W_1}{v_1}\div\dfrac{W_2}{v_2}\quad
F\sim F(v_1,v_2)
\]

Si \(X \sim F(v_1,v_2)\) et \(v_2>2\)
\(E\left[X =\frac{v_2}{v_2-2}\right]\)

\section{Comparer variance échantionnale}\label{stats:stats:f:varuxe9chan}

Soit \(X_1,\dots,X_n\sim N(\mu_1,\sigma^2_1)\) et
\(Y_1,\dots,Y_m\sim N(\mu_2,\sigma^2_2)\) \[
\dfrac{S^2_n}{\sigma^2_1}\div\dfrac{S_m^2}{\sigma^2_2}\sim F(n-1,m-1)
\]

\section{Lemme de Slutsky}\label{lemme-de-slutsky}

Soit \(X_1,X_2,\dots\) et \(Y_1,Y_2,\dots\) Lorsque
\(n \rightarrow \infty\) et \(X_n \rightsquigarrow X\) et
\(Y_n \rightsquigarrow c\)

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \(X_n+Y_n \rightsquigarrow X+c\)
\item
  \(X_n\times Y_n \rightsquigarrow X\times c\)
\item
  Si \(c>0,\; \frac{X_n}{Y_n} \rightsquigarrow \frac{X}{c}\)
\end{enumerate}

\section{Théorème Central Limite}\label{stats:tcl}

Soit \(X_1,\dots,X_n\), un échantillon d'une V.A. quelconque: \[
Z_n=\sqrt{n}\times\frac{\bar{X}_n-\mu}{\sigma}
\] quand \(n \to \infty\): \[
P(Z_n\le X)\to\phi(X)
\] Convergence en distribution: \[
Z_n\rightsquigarrow N(0,1)
\]

\section{Distribution Statistique Student lorsqu'on ne connait pas la variance et X ne provient pas d'une loi Normale}\label{distribution-statistique-student-lorsquon-ne-connait-pas-la-variance-et-x-ne-provient-pas-dune-loi-normale}

\(T \not\sim t(n-1)\)\\
Soit \(X_1,\dots,X_n\), un échantillon d'une V.A quelconque:\\
\(E[X^4]<\infty\), lorsque \(n\to\infty\): \[
T_n = \sqrt{n}\frac{\bar{X}_n-\mu}{S_n}\rightsquigarrow N(0,1)
\]

\section{Aproximation de la loi Binomiale avec la loi Normale}\label{aproximation-de-la-loi-binomiale-avec-la-loi-normale}

\[
Z=\sqrt{n}\times \frac{\bar{X}_n-p}{\sqrt{p(1-p)}}\approx N(0,1)
\] Correction de la continuité: \[
P(Y\le y)\approx P(Z \le y+0.5 )
\]

\section{Critères pour évaluer la performance d'un estimateur}\label{stats:criteres}

\subsection{Critère 1: Biais}\label{stats:criteres:biais}

\begin{gather*}
B(\hat{\theta}_n)= E\left[\hat{\theta}_n-\theta\right]=E\left[\hat{\theta}_n\right]-\theta\\
\intertext{Estimateur sans biais:}\\
B(\hat{\theta}_n)=0\\
\intertext{Estimateur asymptotiquement sans biais:}\\
\lim_{n\to \infty} B(\hat{\theta}_n)=0
\end{gather*}

Voir preuve \ref{preuves:biais:xn} et \ref{preuves:biais:sn} pour le
développement des biais de \(\bar{X}_n\) et \(S_n^2\)

\subsection{Critère 2: Variance}\label{stats:criteres:variance}

Parmi 2 estimateurs sans biais, on préfère celui avec une plus petite
variance.

Pour deux estimateur avec biais, on préfère celui avec la plus petite
Erreur quadratique moyenne(EQM): \[
EQM\left(\hat{\theta}_n\right)= E\left[(\hat{\theta}_n-\theta)^2\right]= Var\left(\hat{\theta}_n\right)+\left[B(\hat{\theta}_n)\right]^2
\]

\subsection{Critère 3: Convergence}\label{stats:convergence}

L'estimateur \(\hat{\theta}_n\) est un estimateur convergent de
\(\theta\) si, quand \(n\to \infty\), \[
\hat{\theta}_n\xrightarrow{P}\theta
\] ce qui signifie que pour tout \(\epsilon>0\), \[
\lim_{n\to\infty}P(|\hat{\theta}-\theta|>\epsilon)=0
\]

Voir \ref{preuves:convergence} pour prouver la convergence

\section{Efficacité relative}\label{stats:effrela}

Soit \(\hat{\theta}_n\) et \(\tilde{\theta}_n\), 2 estimateurs sans
biais et convergent.

\[
\text{eff}(\hat{\theta}_n,\tilde{\theta}_n)= \frac{Var(\tilde{\theta}_n)}{Var(\hat{\theta}_n)}
\] Si \(\text{eff}(\hat{\theta}_n,\tilde{\theta}_n)>1\),
\(\hat{\theta}_n\) est préférable, sinon \(\tilde{\theta}_n\) est
préférable.

\section{Définition formelle statistique exhaustive}\label{definition-formelle-statistique-exhaustive}

Une statistique exhaustive est une statistique qui contient toute
l'information pertinente sur le paramètre visé.

Soit \(X_1,\dots,X_n\) un échantillon aléatoire d'une distribution avec
paramètre \(\theta\) inconnu. Alors, la statistique \[
T=T(X_1,\dots,X_n)
\] est exhaustive pour \(\theta\) ssi la distribution conditionnelle de
\(X_1,\dots,X_n\) sachant \(T\) ne dépend pas de \(\theta\).

\section{Théoreme de factorisation de Fischer-Neyman}\label{theoreme-de-factorisation-de-fischer-neyman}

Soit \(X_1,\dots,X_n\) un échantillon aléatoire d'une distribution avec
densité \(f(\cdot;\theta)\) et paramètre \(\theta\) inconnu. Alors, la
statistique \[
T=T(X_1,\dots,X_n)
\] est exhaustive pour \(\theta\) ssi,
\(\forall\;x_1,\dots,x_n\;\in\mathbb{R}\), \[
f(x_1;\theta)\times\dots\times f(x_n;\theta)=g(t;\theta)\times h(x_1,\dots,x_n)
\] où

\begin{itemize}
\item
  \(g(t;\theta)\) ne dépend de \(x_1,\dots,x_n\) qu'à travers \(t\).
\item
  \(h(x_1,\dots,x_n)\) ne dépend pas de \(\theta\).
\end{itemize}

Avec plus d'un paramètre:

Soit \(X_1,\dots,X_n\) un échantillon aléatoire d'une distribution avec
densité \(f(\cdot;\theta)\) et paramètres
\(\theta=\theta_1,\dots,\theta_n\) inconnus. Alors, les statistiques \[
T_1=T_1(X_1,\dots,X_n),\dots,T_k=T_k(X_1,\dots,X_n)
\] sont conjointements exhaustives pour \(\theta\) ssi,
\(\forall\;x_1,\dots,x_n\;\in\mathbb{R}\), \[
f(x_1;\theta)\times\dots\times f(x_n;\theta)=g(t_1,\dots,t_k;\theta)\times h(x_1,\dots,x_n)
\] où

\begin{itemize}
\item
  \(g(t_1,\dots,t_n;\theta)\) ne dépend de \(x_1,\dots,x_n\) qu'à
  travers \(t_1,\dots,t_k\).
\item
  \(h(x_1,\dots,x_n)\) ne dépend pas de \(\theta\).
\end{itemize}

\section{Critère de Lehmann-Scheffé}\label{critere-de-lehmann-scheffe}

Soit \(X_1,\dots,X_n\) un échantillon aléatoire d'une distribution avec
densité \(f(\cdot;\theta)\) et paramètre \(\theta\) inconnu. Alors, la
statistique \[
T=T(X_1,\dots,X_n)
\] est exhaustive minimale pour \(\theta\) ssi,
\(\forall\;x_1,\dots,x_n,y_1,\dots,y_n\;\in\mathbb{R}\), \[
\frac{f(x_1;\theta)\times\dots\times f(x_n;\theta)}{f(y_1;\theta)\times\dots\times f(y_n;\theta)}
\] ne dépend pas de \(\theta\) ssi \[
T(X_1,\dots,X_n)=T(Y_1,\dots,Y_n)
\]

\section{Théorème de Rao-Blackwell}\label{theoreme-de-rao-blackwell}

\label{stats:rao} \(\hat{\theta}_n\) un estimateur sans biais tel que
\(var(\hat{\theta}_n)<\infty\). Si \(T\) est exhaustive pour \(\theta\),
la statistique: \[
\theta^{*}_n=E[\hat{\theta}_n|T]
\] est un estimateur sans biais et \[
var(\theta^{*}_n)\le var(\hat{\theta}_n)
\] Voir \autoref{preuves:rao}.

\section{Estimateur sans biais et de variance minimale(MVUE)}\label{estimateur-sans-biais-et-de-variance-minimalemvue}

Un estimateur \(\hat{\theta}_n\) est sans biais et de variance minimale
si:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \(\hat{\theta}_n\) est sans biais.
\item
  \(\hat{\theta}_n=g(T)\), où \(T\) est une statistique exhaustive
  (minimale) obtenue avec le théorème Fischer-Neymann.
\end{enumerate}

\subsection{Construire un MVUE}\label{construire-un-mvue}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Trouver une statistique exhaustive (minimale) \(T\) avec le théorème
  Fischer-Neymann.
\item
  Trouver une fonction \(g\) tel que: \(E[g(T)]=\theta\)
\item
  Poser \(\hat{\theta}_n=g(T)\).
\end{enumerate}

\section{Méthode des moments}\label{methode-des-moments}

Si \(t\) paramètres sont inconnus, on résout le système à \(t\)
équations: \[
m_k=E\left[X^k\right],\quad k=1,\dots,t
\] Les estimateurs obtenus sont appelés les estimateurs des moments.

\section{Méthode des quantiles}\label{methode-des-quantiles}

Pour certaine loi, les moments n'existent pas. Pour estimer \(t\)
paramètres inconnus, on pourrait résoudre le système à \(t\) équations:
\[
\hat{\pi}_{\kappa j}=VaR_{\kappa j}(X)\quad j=1,\dots,t
\]

\section{Quantile empirique lissé}\label{quantile-empirique-lisse}

Pour un échantillon aléatoire \(X_1,\dots,X_n\) le quantile empirique de
niveau \(\kappa\in(0,1)\) est: \[
\hat{\pi}_{\kappa,n}=(1-h)X_{(j)}+hX_{(j+1)}\quad j=\lfloor(n+1)\kappa\rfloor\;et \; h=(n+1)\kappa-j 
\]

\section{Fonction de vraissemblance}\label{fonction-de-vraissemblance}

Soit \(X_1,\dots,X_n\) un échantillon aléatoire d'une distribution avec
fmp ou fdd: \[
f(x;\theta),\quad \theta \in \Theta
\] où \(\Theta\) est l'ensemble des valeurs possibles du paramètre. Si
\(x_1,\dots,x_n\) sont des valeurs observées de l'échantillon, la
vraissemblance de \(\theta\) basée sur \(x_1,\dots,x_n\) est définie
comme: \[
L(\theta)=f(x_1;\theta)\times\dots\times f(x_n;\theta)
\]

\subsection*{Observation}\label{observation}

\begin{itemize}
\item
  \(X\) est discrète: vraisemblance de \(\theta\) basée sur
  \(x_1,\dots,x_n\) est exactement la probabilité d'observer
  \(x_1,\dots,x_n\).
\item
  \(X\) est continue: vraisemblance de \(\theta\) basée sur
  \(x_1,\dots,x_n\) est la densité et est proportionelle à la
  probabilité d'observer \(x_1,\dots,x_n\).
\item
  la vraisemblance est vue comme une fonction réelle déterministe de
  \(\theta\)
\item
  La vraisemblance est l'objet dans le théorème de factorisation de
  Fischer-Neymann
\item
  la vraisemblance \(L(\theta)\) devrait être plus grande pour des
  valeurs de \(\theta\) proche de celle du mécanisme générateur de
  données.
\item
  on estime donc \(\theta\) par la valeur \(\hat{\theta}_n\) qui
  maximise \(L(\theta)\): \[
  \hat{\theta}_n=\underset{\theta\in\Theta}{arg\,max}\,L(\theta) 
  \]
\end{itemize}

\section{Estimateur du maximum de vraissemblance}\label{estimateur-du-maximum-de-vraissemblance}

Soit \(x_1,\dots,x_n\), les valeurs observées d'un échantillon aléatoire
de \(f(x;\theta)\), où \(\theta=(\theta_1,\dots,\theta_k)\) sont des
paramètres inconnus (\(\in \Theta\), l'espace des paramètres). La valeur
observée de l'EMV de \(\theta\) est la valeur \(\hat{\theta}\) qui
maximise la vraisemblance de \(\theta\) basée sur \(x_1,\dots,x_n\) \[
\hat{\theta}_n=\underset{\theta\in\Theta}{argmax}\,L(\theta)
\] On suppose que le max est unique.

Les EMVs sont basés sur des statistiques exhaustives, sont souvent
biaisés, mais habituellement asymptotiquement sans biais. On obtient
souvent des MVUEs lorsqu'on corrige le biais.

\section{Log-vraisemblance}\label{log-vraisemblance}

\[
\ell(\theta)=ln\left (L(\theta)\right )=\sum^n_{i=1} ln\left (f(x_i;\theta)\right )
\] Cette fonction est strictement croissante. Ainsi, l'EMV peut être
obtenu en maximisant la log-vraisemblance. \[
\hat{\theta}=\underset{\theta\in\Theta}{argmax}\,\ell(\theta)
\]

\subsection{EMV et exhaustivité}\label{emv-et-exhaustivite}

Avec le théorème de Fischer-Neymann \[
L(\theta)=g(t,\theta)\,h(x_1,\dots,x_n)
\]\\
et la log-vraisemblance \[
\ell(\theta)=ln(g(t,\theta))+ln(h(x_1,\dots,x_n))
\]

Ainsi, puisque \(h(x_1,\dots,x_n)\) ne dépend pas de \(\theta\),
l'estimation du MV est \[
\underset{\theta\in\Theta}{argmax}\, ln(g(t,\theta))
\] une fonction de la valeur \(t\) de la statistique exhaustive \(T\).

\section{Propriété de l'EMV pour de grands échantillons}\label{propriete-de-lemv-pour-de-grands-echantillons}

Sous certaines conditions de régularité sur la fonction de masse de
probabilté ou de la densité, l'EMV \(\hat{\theta}\) existe et est unique
avec probabilité convergeant vers 1 lorsque \(n\rightarrow \infty\) \[
\dfrac{\hat{\theta}_n-\theta}{\sqrt{\dfrac{1}{nI(\theta)}}}\leadsto N(0,1)
\] quand,\(n\rightarrow\infty\).

\subsubsection*{Information de Fischer}\label{information-de-fischer}
\addcontentsline{toc}{subsubsection}{Information de Fischer}

\[
I(\theta)=\text{E}\left [\left (\dfrac{\partial}{\partial \theta}ln f(X;\theta)\right )^2\right ]
\]

On peut aussi formuler l'EMV pour les grandes valeurs de n comme \[
\hat{\theta}\approx N\left(\theta,\dfrac{1}{nI(\theta)}\right)
\] et

\begin{align*}
I(\theta)& =\text{E}\left [\left (\dfrac{\partial}{\partial \theta}ln f(X;\theta)\right )^2\right ]\\
& =\text{E}\left [-\dfrac{{\partial}^2}{\partial {\theta}^2}ln f(X;\theta)\right ]
\end{align*}

Si les conditions de régularité sont vérifées, l'EMV est convergent \[
\hat{\theta}_n\leadsto \theta
\] et \[
\lim_{n\rightarrow\infty} var(\hat{\theta}_n)=0
\]

On peut montrer que tout estimateur sans biais \(\hat{\theta}\)
satisfait l'inégalité de Cramer-Rao: \[
var(\hat{\theta}) \geq \dfrac{1}{nI(\theta)}
\]

L'EMV est donc asymptotiquement efficace. Il a la plus petite variance
possible, asymptotiquement. On retrouve des formes fermées pour les EMVs
dans les cas simples seulement. On doit obtenir les autres par des
techniques d'optimisation numérique. La fonction de log-vraisemblance
est convexe et lisse, ce qui facilite l'optimisation. On utilise les
estimateurs des moments comme valeurs de départs dans les fonctions
d'optimisation.

\section{Propriété d'invariance de l'EMV}\label{propriete-dinvariance-de-lemv}

Supposons que le paramètre d'intérèt est \(\lambda=g(\theta)\), où \(g\)
est une fonction bijective sur \(\Theta\). Puisque \[
L(\theta)=L(g^{-1}(\lambda))
\] le maximum est atteint à \[
\hat{\theta}_n=g^{-1}(\hat{\lambda}_n)\Leftrightarrow \hat{\lambda}_n=g(\hat{\theta}_n)
\] où \(\hat{\theta}_n\) est l'EMV de \(\theta\).

\section{Diagramme quantile-quantile}\label{diagramme-quantile-quantile}

Outil pour vérifier l'ajustement d'un modèle.

Si \(x_1,\dots,x_n\) sont les données d'un modèle paramétrique qu'on a
ajusté avec fonction de répartition \(F(\cdot;\theta)\). Le diagramme
Q-Q comprend les points \[
\left (F^{-1}\left(\dfrac{1}{n+1};\hat{\theta}_n\right),x_{(i)}\right ),\;i=1,\dots,n
\] où \(x_{(1)} \leq \dots \leq x_{(n)}\) sont les observations
ordonnées de \(x_1,\dots,x_n\).

Avec ce diagramme, on compare les quantiles empiriques avec les
quantiles théoriques. Si le modèle est bien ajusté, les points devraient
formés une droite. S'ils forment une courbe vers le haut/bas, les
quantiles théoriques sont trop petits/grands.

\section{Critère d'information d'Akaike}\label{critere-dinformation-dakaike}

Soit un modèle paramétrique pour les données, avec paramètre \[
\theta=(\theta_1,\dots,\theta_k) 
\]\\
\(\hat{\theta}_n\) est l'EMV de \(\theta\). Le critère d'information
d'Akaike est \[
AIC = 2(-\ell(\hat{\theta}_n)+k)
\] On préfère le modèle avec le plus petit \(AIC\)

\section{Critère d'information bayésien de Schwartz}\label{critere-dinformation-bayesien-de-schwartz}

\[
BIC = -2\ell(\hat{\theta}_n)+k ln(n)
\]


\section{Inteval de confiance bilatéral}

Définition:
Pour $\alpha \in (0,1)$ et un paramètre $\theta \in \Theta \subset \mathrm{R}$, les stats $\hat{\theta}_L \text{ et } \hat{\theta}_U$ sont appelées les bornes inf. et sup. d'un intervalle de confiance, si 
\[\Pr(\hat{\theta}_L \leq \theta \leq \hat{\theta}_U
) = 1- \alpha\].

On a donc un intervalle \emph{aléatoire}:
\[ [\hat{\theta}_L,\hat{\theta}_U
]\]

appelé intervalle de confiance bilatéral,$1-\alpha$ est le miveau de confiance

\section{Intervalle de confiance unilatéral}

Intervalle de confiance unilatéral à gauche:
\[\Pr(\hat{\theta}_{L} \leq \theta)=1-\alpha\]  

Intervalle de confiance unilatéral à droite:
\[\Pr(\theta \leq \hat{\theta}_{U})=1-\alpha\]

Remarque sur les intervalles de confiance
\begin{enumerate}
\item si $n$ augmente, l'intervalledevient plus petit
\item si $1- \alpha$ augmente, l'intervalle devient plus grand
\end{enumerate}

\section{Méthode du pivot}

 \begin{enumerate}
 \item Pivot: une fonction des obsevations et du paramètre inconnu qui nous intéresse
 dont la distribution est entièrement connue et ne dépend pas de $\theta$. 
 \item Trouver les quantiles appropriés de la distibution $W$ pour un intervalle bilatéral
 \item On calcule l'intervalle:
 	\[w_{1-\alpha/2} \leq W(X_1, \dots X_n, \theta) \leq w_{\alpha/2}\]
 \end{enumerate}
 
\section{Intervalle de confiance pour échantillons de la loi normale}

Soit $X_1, \dots X_n \sim N(\mu, \sigma^2)$ avec paramètres inconnus.

\subsection*{Intervalle de confiance pour $\mu$}

Voir \autoref{stats:dist:student} pour la distribution de la statistique T.

$T_n$ est un pivot et on a 
\[\Pr(-t_{n-1,\alpha/2} \leq \sqrt{n}\dfrac{\bar{X}_n- \mu}{S_n} \leq t_{n-1,\alpha/2}\].
L'intervalle de confiance est:
\[\left [ \bar{X}_n - t_{n-1,\alpha/2} \dfrac{S_n}{\sqrt{n}},\bar{X}_n + t_{n-1,\alpha/2} \dfrac{S_n}{\sqrt{n}}\right ] \]

\subsection*{Différence entre 2 moyennes}

Pour comparer 2 moyennes.

On obtient facilement des intervalles de confiance lorsqu'on a les conditions suivantes:

\begin{itemize}
\item $X_1, \dots X_n \sim N(\mu, \sigma^2)$
\item $Y_1, \dots Y_n \sim N(\mu, \sigma^2)$
\item les 2 échantillons sont mutuellement indépendants
\item $\sigma^2_1=\sigma^2_2 =\sigma^2$
\end{itemize}

Ainsi, on a,
\begin{align*}
\dfrac{(\bar{X}_n-\bar{Y}_n)-(\mu_1-\mu_2)}{\sqrt{\dfrac{\sigma^2}{n}+ \dfrac{\sigma^2}{m}}}& =\dfrac{(\bar{X}_n-\bar{Y}_n)-(\mu_1-\mu_2)}{\sigma \sqrt{\dfrac{1}{n}+\dfrac{1}{m}}}\sim N(0,1)\\
\text{Il faut estimer la variance commune}\\
S^2& = \dfrac{n-1}{n+m-2}S^2_n+\dfrac{m-1}{n+m-2}S^2_m\\
& = \dfrac{1}{n+m-2} \left \{ \sum^n_{i=1} (X_i-\bar{X}_n)^2 + \sum^m_{j=1} (Y_j- \bar{Y}_n)^2 \right \} \\
S^2_n \text{ et } S^2_m \text{ sont indépendantes}\\
\dfrac{n+m-2}{\sigma^2}S^2& =\dfrac{n-1}{\sigma^2}S^2_n+ \dfrac{m-1}{\sigma^2} S_m^2 \sim \chi^2_{(n+m-2)}\\
\text{Ainsi,}\\
\dfrac{\bar{X}_n-\bar{Y}_n)-(\mu_1-\mu_2}{S \sqrt{\dfrac{1}{n} + \dfrac{1}{m}}} &\sim t_{(n+m-2)}
\end{align*}

L'intervalle de confiance pour la différence de moyennes:
 
\[ \left [ (\bar{X}_n - \bar{Y}_n) -t_{(n+m-2,\alpha/2)} S \sqrt{\dfrac{1}{n} +\dfrac{1}{m}},(\bar{X}_n - \bar{Y}_n) -t_{(n+m-2,\alpha/2)} S \sqrt{\dfrac{1}{n} +\dfrac{1}{m}} \right ]  \]

\subsection*{Intervalle de confiance pour la variance}\label{stats:intconfvar}

 
Le pivot à utiliser:
\[\dfrac{n-1}{\sigma^2}S^2_n=\dfrac{1}{\sigma^2}`\sum^n_{i=1} (X_i-\bar{X}_n)^2 \sim \chi^2(n-1)\]

L'intervalle sera:

\[\left [ \dfrac{n-1}{\chi^2_{n-1,\alpha/2}}S_n^2,\dfrac{n-1}{\chi^2_{n-1,1-\alpha/2}}S_n^2 \right ] \]


 